"use strict";
// The require scope
var __webpack_require__ = {};
/************************************************************************/ // webpack/runtime/compat_get_default_export
(()=>{
    // getDefaultExport function for compatibility with non-ESM modules
    __webpack_require__.n = function(module) {
        var getter = module && module.__esModule ? function() {
            return module['default'];
        } : function() {
            return module;
        };
        __webpack_require__.d(getter, {
            a: getter
        });
        return getter;
    };
})();
// webpack/runtime/define_property_getters
(()=>{
    __webpack_require__.d = function(exports1, definition) {
        for(var key in definition)if (__webpack_require__.o(definition, key) && !__webpack_require__.o(exports1, key)) Object.defineProperty(exports1, key, {
            enumerable: true,
            get: definition[key]
        });
    };
})();
// webpack/runtime/has_own_property
(()=>{
    __webpack_require__.o = function(obj, prop) {
        return Object.prototype.hasOwnProperty.call(obj, prop);
    };
})();
// webpack/runtime/make_namespace_object
(()=>{
    // define __esModule on exports
    __webpack_require__.r = function(exports1) {
        if ('undefined' != typeof Symbol && Symbol.toStringTag) Object.defineProperty(exports1, Symbol.toStringTag, {
            value: 'Module'
        });
        Object.defineProperty(exports1, '__esModule', {
            value: true
        });
    };
})();
/************************************************************************/ var __webpack_exports__ = {};
// ESM COMPAT FLAG
__webpack_require__.r(__webpack_exports__);
// EXPORTS
__webpack_require__.d(__webpack_exports__, {
    WsChatClient: ()=>/* reexport */ ws_tools_chat,
    AIDenoiserProcessorLevel: ()=>/* reexport */ pcm_recorder_AIDenoiserProcessorLevel,
    AIDenoiserProcessorMode: ()=>/* reexport */ pcm_recorder_AIDenoiserProcessorMode,
    WsChatEventNames: ()=>/* reexport */ types_WsChatEventNames,
    WsToolsUtils: ()=>/* reexport */ utils_namespaceObject,
    PcmRecorder: ()=>/* reexport */ pcm_recorder,
    WsTranscriptionClient: ()=>/* reexport */ transcription,
    WsSpeechClient: ()=>/* reexport */ speech
});
// NAMESPACE OBJECT: ./src/ws-tools/utils/index.ts
var utils_namespaceObject = {};
__webpack_require__.r(utils_namespaceObject);
__webpack_require__.d(utils_namespaceObject, {
    checkDenoiserSupport: ()=>checkDenoiserSupport,
    checkDevicePermission: ()=>checkDevicePermission,
    floatTo16BitPCM: ()=>floatTo16BitPCM,
    getAudioDevices: ()=>getAudioDevices,
    isMobile: ()=>isMobile
});
const external_agora_rtc_sdk_ng_namespaceObject = require("agora-rtc-sdk-ng");
var external_agora_rtc_sdk_ng_default = /*#__PURE__*/ __webpack_require__.n(external_agora_rtc_sdk_ng_namespaceObject);
const external_agora_extension_ai_denoiser_namespaceObject = require("agora-extension-ai-denoiser");
/**
 * Check audio device permissions
 * @returns {Promise<{audio: boolean}>} Whether audio device permission is granted
 */ const checkDevicePermission = async ()=>{
    const result = {
        audio: true
    };
    try {
        // Check if browser supports mediaDevices API
        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
            console.error('Browser does not support mediaDevices API');
            result.audio = false;
        }
        // Check permission status first through permissions API
        const permissionStatus = await navigator.permissions.query({
            name: 'microphone'
        });
        // 如果权限已被拒绝
        if ('denied' === permissionStatus.state) {
            console.error('Microphone permission denied');
            result.audio = false;
        }
        // If permission status is prompt or granted, try to get device
        if ('prompt' === permissionStatus.state || 'granted' === permissionStatus.state) {
            const stream = await navigator.mediaDevices.getUserMedia({
                audio: true
            });
            // 获取成功后，关闭音频流
            if (stream) stream.getTracks().forEach((track)=>track.stop());
        }
    } catch (error) {
        // 用户拒绝授权或其他错误
        console.error('Failed to get audio permission:', error);
        result.audio = false;
    }
    return result;
};
/**
 * Get list of audio devices
 * @returns {Promise<{audioInputs: MediaDeviceInfo[], audioOutputs: MediaDeviceInfo[]}>} Audio devices
 */ const getAudioDevices = async ()=>{
    try {
        // request microphone permission first, so we can get the complete device information
        const { audio: audioPermission } = await checkDevicePermission();
        if (!audioPermission) throw new Error('Microphone permission denied');
        // get all media devices
        const devices = await navigator.mediaDevices.enumerateDevices();
        if (!(null == devices ? void 0 : devices.length)) return {
            audioInputs: [],
            audioOutputs: []
        };
        return {
            audioInputs: devices.filter((i)=>i.deviceId && 'audioinput' === i.kind),
            audioOutputs: devices.filter((i)=>i.deviceId && 'audiooutput' === i.kind)
        };
    } catch (error) {
        console.error('Failed to get audio devices:', error);
        return {
            audioInputs: [],
            audioOutputs: []
        };
    }
};
/**
 * Convert floating point numbers to 16-bit PCM
 * @param float32Array - Array of floating point numbers
 * @returns {ArrayBuffer} 16-bit PCM
 */ const floatTo16BitPCM = (float32Array)=>{
    const buffer = new ArrayBuffer(2 * float32Array.length);
    const view = new DataView(buffer);
    let offset = 0;
    for(let i = 0; i < float32Array.length; i++, offset += 2){
        const s = Math.max(-1, Math.min(1, float32Array[i]));
        view.setInt16(offset, s < 0 ? 0x8000 * s : 0x7fff * s, true);
    }
    return buffer;
};
/**
 * Check if device is mobile
 * @returns {boolean} Whether device is mobile
 */ const isMobile = ()=>/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
/**
 * Check if AI denoising is supported
 * @param assetsPath - Public path for denoising plugin
 * @returns {boolean} Whether AI denoising is supported
 */ const checkDenoiserSupport = (assetsPath)=>{
    if (!window.__denoiser) {
        // 传入 Wasm 文件所在的公共路径以创建 AIDenoiserExtension 实例，路径结尾不带 / "
        const external = new external_agora_extension_ai_denoiser_namespaceObject.AIDenoiserExtension({
            assetsPath: null != assetsPath ? assetsPath : 'https://lf3-static.bytednsdoc.com/obj/eden-cn/613eh7lpqvhpeuloz/websocket'
        });
        window.__denoiser = external;
        external.onloaderror = (e)=>{
            // 如果 Wasm 文件加载失败，你可以关闭插件，例如：
            console.error('Denoiser load error', e);
        };
        // 检查兼容性
        if (external.checkCompatibility()) {
            // 注册插件
            // see https://github.com/AgoraIO/API-Examples-Web/blob/main/src/example/extension/aiDenoiser/agora-extension-ai-denoiser/README.md
            external_agora_rtc_sdk_ng_default().registerExtensions([
                external
            ]);
            return true;
        }
        // 当前浏览器可能不支持 AI 降噪插件，你可以停止执行之后的逻辑
        console.error('Does not support AI Denoiser!');
        return false;
    }
    return window.__denoiser.checkCompatibility();
};
const external_uuid_namespaceObject = require("uuid");
const StreamProcessorWorklet = `
class StreamProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
    this.hasStarted = false;
    this.hasInterrupted = false;
    this.outputBuffers = [];
    this.bufferLength = 128;
    this.write = { buffer: new Float32Array(this.bufferLength), trackId: null };
    this.writeOffset = 0;
    this.trackSampleOffsets = {};
    this.port.onmessage = (event) => {
      if (event.data) {
        const payload = event.data;
        if (payload.event === 'write') {
          const int16Array = payload.buffer;
          const float32Array = new Float32Array(int16Array.length);
          for (let i = 0; i < int16Array.length; i++) {
            float32Array[i] = int16Array[i] / 0x8000; // Convert Int16 to Float32
          }
          this.writeData(float32Array, payload.trackId);
        } else if (
          payload.event === 'offset' ||
          payload.event === 'interrupt'
        ) {
          const requestId = payload.requestId;
          const trackId = this.write.trackId;
          const offset = this.trackSampleOffsets[trackId] || 0;
          this.port.postMessage({
            event: 'offset',
            requestId,
            trackId,
            offset,
          });
          if (payload.event === 'interrupt') {
            this.hasInterrupted = true;
          }
        } else {
          throw new Error(\`Unhandled event "\${payload.event}"\`);
        }
      }
    };
  }

  writeData(float32Array, trackId = null) {
    let { buffer } = this.write;
    let offset = this.writeOffset;
    for (let i = 0; i < float32Array.length; i++) {
      buffer[offset++] = float32Array[i];
      if (offset >= buffer.length) {
        this.outputBuffers.push(this.write);
        this.write = { buffer: new Float32Array(this.bufferLength), trackId };
        buffer = this.write.buffer;
        offset = 0;
      }
    }
    this.writeOffset = offset;
    return true;
  }

  process(inputs, outputs, parameters) {
    const output = outputs[0];
    const outputChannelData = output[0];
    const outputBuffers = this.outputBuffers;
    if (this.hasInterrupted) {
      this.port.postMessage({ event: 'stop' });
      return false;
    } else if (outputBuffers.length) {
      this.hasStarted = true;
      const { buffer, trackId } = outputBuffers.shift();
      for (let i = 0; i < outputChannelData.length; i++) {
        outputChannelData[i] = buffer[i] || 0;
      }
      if (trackId) {
        this.trackSampleOffsets[trackId] =
          this.trackSampleOffsets[trackId] || 0;
        this.trackSampleOffsets[trackId] += buffer.length;
      }
      return true;
    } else if (this.hasStarted) {
      this.port.postMessage({ event: 'stop' });
      return false;
    } else {
      return true;
    }
  }
}

registerProcessor('stream_processor', StreamProcessor);
`;
const script = new Blob([
    StreamProcessorWorklet
], {
    type: 'application/javascript'
});
const src = URL.createObjectURL(script);
const StreamProcessorSrc = src;
/**
 * Constants for help with visualization
 * Helps map frequency ranges from Fast Fourier Transform
 * to human-interpretable ranges, notably music ranges and
 * human vocal ranges.
 */ // Eighth octave frequencies
const octave8Frequencies = [
    4186.01,
    4434.92,
    4698.63,
    4978.03,
    5274.04,
    5587.65,
    5919.91,
    6271.93,
    6644.88,
    7040.0,
    7458.62,
    7902.13
];
// Labels for each of the above frequencies
const octave8FrequencyLabels = [
    'C',
    'C#',
    'D',
    'D#',
    'E',
    'F',
    'F#',
    'G',
    'G#',
    'A',
    'A#',
    'B'
];
/**
 * All note frequencies from 1st to 8th octave
 * in format "A#8" (A#, 8th octave)
 */ const noteFrequencies = [];
const noteFrequencyLabels = [];
for(let i = 1; i <= 8; i++)for(let f = 0; f < octave8Frequencies.length; f++){
    const freq = octave8Frequencies[f];
    noteFrequencies.push(freq / Math.pow(2, 8 - i));
    noteFrequencyLabels.push(octave8FrequencyLabels[f] + i);
}
/**
 * Subset of the note frequencies between 32 and 2000 Hz
 * 6 octave range: C1 to B6
 */ const voiceFrequencyRange = [
    32.0,
    2000.0
];
const voiceFrequencies = noteFrequencies.filter((_, i)=>noteFrequencies[i] > voiceFrequencyRange[0] && noteFrequencies[i] < voiceFrequencyRange[1]);
const voiceFrequencyLabels = noteFrequencyLabels.filter((_, i)=>noteFrequencies[i] > voiceFrequencyRange[0] && noteFrequencies[i] < voiceFrequencyRange[1]);
/**
 * Output of AudioAnalysis for the frequency domain of the audio
 * @typedef {Object} AudioAnalysisOutputType
 * @property {Float32Array} values Amplitude of this frequency between {0, 1} inclusive
 * @property {number[]} frequencies Raw frequency bucket values
 * @property {string[]} labels Labels for the frequency bucket values
 */ /**
 * Analyzes audio for visual output
 * @class
 */ class AudioAnalysis {
    /**
   * Retrieves frequency domain data from an AnalyserNode adjusted to a decibel range
   * returns human-readable formatting and labels
   * @param {AnalyserNode} analyser
   * @param {number} sampleRate
   * @param {Float32Array} [fftResult]
   * @param {"frequency"|"music"|"voice"} [analysisType]
   * @param {number} [minDecibels] default -100
   * @param {number} [maxDecibels] default -30
   * @returns {AudioAnalysisOutputType}
   */ static getFrequencies(analyser, sampleRate, fftResult) {
        let analysisType = arguments.length > 3 && void 0 !== arguments[3] ? arguments[3] : 'frequency', minDecibels = arguments.length > 4 && void 0 !== arguments[4] ? arguments[4] : -100, maxDecibels = arguments.length > 5 && void 0 !== arguments[5] ? arguments[5] : -30;
        if (!fftResult) {
            fftResult = new Float32Array(analyser.frequencyBinCount);
            analyser.getFloatFrequencyData(fftResult);
        }
        const nyquistFrequency = sampleRate / 2;
        const frequencyStep = 1 / fftResult.length * nyquistFrequency;
        let outputValues;
        let frequencies;
        let labels;
        if ('music' === analysisType || 'voice' === analysisType) {
            const useFrequencies = 'voice' === analysisType ? voiceFrequencies : noteFrequencies;
            const aggregateOutput = Array(useFrequencies.length).fill(minDecibels);
            for(let i = 0; i < fftResult.length; i++){
                const frequency = i * frequencyStep;
                const amplitude = fftResult[i];
                for(let n = useFrequencies.length - 1; n >= 0; n--)if (frequency > useFrequencies[n]) {
                    aggregateOutput[n] = Math.max(aggregateOutput[n], amplitude);
                    break;
                }
            }
            outputValues = aggregateOutput;
            frequencies = 'voice' === analysisType ? voiceFrequencies : noteFrequencies;
            labels = 'voice' === analysisType ? voiceFrequencyLabels : noteFrequencyLabels;
        } else {
            outputValues = Array.from(fftResult);
            frequencies = outputValues.map((_, i)=>frequencyStep * i);
            labels = frequencies.map((f)=>`${f.toFixed(2)} Hz`);
        }
        // We normalize to {0, 1}
        const normalizedOutput = outputValues.map((v)=>Math.max(0, Math.min((v - minDecibels) / (maxDecibels - minDecibels), 1)));
        const values = new Float32Array(normalizedOutput);
        return {
            values,
            frequencies,
            labels
        };
    }
    /**
   * Gets the current frequency domain data from the playing audio track
   * @param {"frequency"|"music"|"voice"} [analysisType]
   * @param {number} [minDecibels] default -100
   * @param {number} [maxDecibels] default -30
   * @returns {AudioAnalysisOutputType}
   */ getFrequencies() {
        let analysisType = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : 'frequency', minDecibels = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : -100, maxDecibels = arguments.length > 2 && void 0 !== arguments[2] ? arguments[2] : -30;
        let fftResult = null;
        if (this.audioBuffer && this.fftResults.length) {
            const pct = this.audio.currentTime / this.audio.duration;
            const index = Math.min(pct * this.fftResults.length | 0, this.fftResults.length - 1);
            fftResult = this.fftResults[index];
        }
        return AudioAnalysis.getFrequencies(this.analyser, this.sampleRate, fftResult, analysisType, minDecibels, maxDecibels);
    }
    /**
   * Resume the internal AudioContext if it was suspended due to the lack of
   * user interaction when the AudioAnalysis was instantiated.
   * @returns {Promise<true>}
   */ async resumeIfSuspended() {
        if ('suspended' === this.context.state) await this.context.resume();
        return true;
    }
    /**
   * Creates a new AudioAnalysis instance for an HTMLAudioElement
   * @param {HTMLAudioElement} audioElement
   * @param {AudioBuffer|null} [audioBuffer] If provided, will cache all frequency domain data from the buffer
   * @returns {AudioAnalysis}
   */ constructor(audioElement, audioBuffer = null){
        this.fftResults = [];
        if (audioBuffer) {
            /**
       * Modified from
       * https://stackoverflow.com/questions/75063715/using-the-web-audio-api-to-analyze-a-song-without-playing
       *
       * We do this to populate FFT values for the audio if provided an `audioBuffer`
       * The reason to do this is that Safari fails when using `createMediaElementSource`
       * This has a non-zero RAM cost so we only opt-in to run it on Safari, Chrome is better
       */ const { length, sampleRate } = audioBuffer;
            const offlineAudioContext = new OfflineAudioContext({
                length,
                sampleRate
            });
            const source = offlineAudioContext.createBufferSource();
            source.buffer = audioBuffer;
            const analyser = offlineAudioContext.createAnalyser();
            analyser.fftSize = 8192;
            analyser.smoothingTimeConstant = 0.1;
            source.connect(analyser);
            // limit is :: 128 / sampleRate;
            // but we just want 60fps - cuts ~1s from 6MB to 1MB of RAM
            const renderQuantumInSeconds = 1 / 60;
            const durationInSeconds = length / sampleRate;
            const analyze = (index)=>{
                const suspendTime = renderQuantumInSeconds * index;
                if (suspendTime < durationInSeconds) offlineAudioContext.suspend(suspendTime).then(()=>{
                    const fftResult = new Float32Array(analyser.frequencyBinCount);
                    analyser.getFloatFrequencyData(fftResult);
                    this.fftResults.push(fftResult);
                    analyze(index + 1);
                });
                if (1 === index) offlineAudioContext.startRendering();
                else offlineAudioContext.resume();
            };
            source.start(0);
            analyze(1);
            this.audio = audioElement;
            this.context = offlineAudioContext;
            this.analyser = analyser;
            this.sampleRate = sampleRate;
            this.audioBuffer = audioBuffer;
        } else {
            const audioContext = new AudioContext();
            const track = audioContext.createMediaElementSource(audioElement);
            const analyser = audioContext.createAnalyser();
            analyser.fftSize = 8192;
            analyser.smoothingTimeConstant = 0.1;
            track.connect(analyser);
            analyser.connect(audioContext.destination);
            this.audio = audioElement;
            this.context = audioContext;
            this.analyser = analyser;
            this.sampleRate = this.context.sampleRate;
            this.audioBuffer = null;
        }
    }
}
globalThis.AudioAnalysis = AudioAnalysis;
/**
 * Plays audio streams received in raw PCM16 chunks from the browser
 * @class
 */ class WavStreamPlayer {
    /**
   * Connects the audio context and enables output to speakers
   * @returns {Promise<true>}
   */ async connect() {
        this.context = new AudioContext({
            sampleRate: this.sampleRate
        });
        if ('suspended' === this.context.state) await this.context.resume();
        try {
            await this.context.audioWorklet.addModule(this.scriptSrc);
        } catch (e) {
            console.error(e);
            throw new Error(`Could not add audioWorklet module: ${this.scriptSrc}`);
        }
        const analyser = this.context.createAnalyser();
        analyser.fftSize = 8192;
        analyser.smoothingTimeConstant = 0.1;
        this.analyser = analyser;
        return true;
    }
    async pause() {
        if (this.context && !this.isPaused) {
            await this.context.suspend();
            this.isPaused = true;
        }
    }
    async resume() {
        if (this.context && this.isPaused) {
            await this.context.resume();
            this.isPaused = false;
        }
    }
    async togglePlay() {
        if (this.isPaused) await this.resume();
        else await this.pause();
    }
    isPlaying() {
        return this.context && this.stream && !this.isPaused && 'running' === this.context.state;
    }
    /**
   * Gets the current frequency domain data from the playing track
   * @param {"frequency"|"music"|"voice"} [analysisType]
   * @param {number} [minDecibels] default -100
   * @param {number} [maxDecibels] default -30
   * @returns {import('./analysis/audio_analysis.js').AudioAnalysisOutputType}
   */ getFrequencies() {
        let analysisType = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : 'frequency', minDecibels = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : -100, maxDecibels = arguments.length > 2 && void 0 !== arguments[2] ? arguments[2] : -30;
        if (!this.analyser) throw new Error('Not connected, please call .connect() first');
        return AudioAnalysis.getFrequencies(this.analyser, this.sampleRate, null, analysisType, minDecibels, maxDecibels);
    }
    /**
   * Starts audio streaming
   * @private
   * @returns {Promise<true>}
   */ async _start() {
        // Ensure worklet is loaded
        if (!this.context) await this.connect();
        const streamNode = new AudioWorkletNode(this.context, 'stream_processor');
        streamNode.connect(this.context.destination);
        streamNode.port.onmessage = (e)=>{
            const { event } = e.data;
            if ('stop' === event) {
                streamNode.disconnect();
                this.stream = null;
            } else if ('offset' === event) {
                const { requestId, trackId, offset } = e.data;
                const currentTime = offset / this.sampleRate;
                this.trackSampleOffsets[requestId] = {
                    trackId,
                    offset,
                    currentTime
                };
            }
        };
        this.analyser.disconnect();
        streamNode.connect(this.analyser);
        this.stream = streamNode;
        return true;
    }
    /**
   * Adds 16BitPCM data to the currently playing audio stream
   * You can add chunks beyond the current play point and they will be queued for play
   * @param {ArrayBuffer|Int16Array} arrayBuffer
   * @param {string} [trackId]
   * @returns {Int16Array}
   */ async add16BitPCM(arrayBuffer) {
        let trackId = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : 'default';
        if ('string' != typeof trackId) throw new Error("trackId must be a string");
        if (this.interruptedTrackIds[trackId]) return;
        if (!this.stream) await this._start();
        let buffer;
        if (arrayBuffer instanceof Int16Array) buffer = arrayBuffer;
        else if (arrayBuffer instanceof ArrayBuffer) buffer = new Int16Array(arrayBuffer);
        else throw new Error("argument must be Int16Array or ArrayBuffer");
        this.stream.port.postMessage({
            event: 'write',
            buffer,
            trackId
        });
        return buffer;
    }
    /**
   * Gets the offset (sample count) of the currently playing stream
   * @param {boolean} [interrupt]
   * @returns {{trackId: string|null, offset: number, currentTime: number}}
   */ async getTrackSampleOffset() {
        let interrupt = arguments.length > 0 && void 0 !== arguments[0] && arguments[0];
        if (!this.stream) return null;
        const requestId = crypto.randomUUID();
        this.stream.port.postMessage({
            event: interrupt ? 'interrupt' : 'offset',
            requestId
        });
        let trackSampleOffset;
        while(!trackSampleOffset){
            trackSampleOffset = this.trackSampleOffsets[requestId];
            await new Promise((r)=>setTimeout(()=>r(), 1));
        }
        const { trackId } = trackSampleOffset;
        if (interrupt && trackId) this.interruptedTrackIds[trackId] = true;
        return trackSampleOffset;
    }
    /**
   * Strips the current stream and returns the sample offset of the audio
   * @param {boolean} [interrupt]
   * @returns {{trackId: string|null, offset: number, currentTime: number}}
   */ async interrupt() {
        return this.getTrackSampleOffset(true);
    }
    /**
   * Creates a new WavStreamPlayer instance
   * @param {{sampleRate?: number}} options
   * @returns {WavStreamPlayer}
   */ constructor({ sampleRate = 44100 } = {}){
        this.scriptSrc = StreamProcessorSrc;
        this.sampleRate = sampleRate;
        this.context = null;
        this.stream = null;
        this.analyser = null;
        this.trackSampleOffsets = {};
        this.interruptedTrackIds = {};
        this.isPaused = false;
    }
}
globalThis.WavStreamPlayer = WavStreamPlayer;
const AudioProcessorWorklet = `
class AudioProcessor extends AudioWorkletProcessor {

  constructor() {
    super();
    this.port.onmessage = this.receive.bind(this);
    this.initialize();
  }

  initialize() {
    this.foundAudio = false;
    this.recording = false;
    this.chunks = [];
  }

  /**
   * Concatenates sampled chunks into channels
   * Format is chunk[Left[], Right[]]
   */
  readChannelData(chunks, channel = -1, maxChannels = 9) {
    let channelLimit;
    if (channel !== -1) {
      if (chunks[0] && chunks[0].length - 1 < channel) {
        throw new Error(
          \`Channel \${channel} out of range: max \${chunks[0].length}\`
        );
      }
      channelLimit = channel + 1;
    } else {
      channel = 0;
      channelLimit = Math.min(chunks[0] ? chunks[0].length : 1, maxChannels);
    }
    const channels = [];
    for (let n = channel; n < channelLimit; n++) {
      const length = chunks.reduce((sum, chunk) => {
        return sum + chunk[n].length;
      }, 0);
      const buffers = chunks.map((chunk) => chunk[n]);
      const result = new Float32Array(length);
      let offset = 0;
      for (let i = 0; i < buffers.length; i++) {
        result.set(buffers[i], offset);
        offset += buffers[i].length;
      }
      channels[n] = result;
    }
    return channels;
  }

  /**
   * Combines parallel audio data into correct format,
   * channels[Left[], Right[]] to float32Array[LRLRLRLR...]
   */
  formatAudioData(channels) {
    if (channels.length === 1) {
      // Simple case is only one channel
      const float32Array = channels[0].slice();
      const meanValues = channels[0].slice();
      return { float32Array, meanValues };
    } else {
      const float32Array = new Float32Array(
        channels[0].length * channels.length
      );
      const meanValues = new Float32Array(channels[0].length);
      for (let i = 0; i < channels[0].length; i++) {
        const offset = i * channels.length;
        let meanValue = 0;
        for (let n = 0; n < channels.length; n++) {
          float32Array[offset + n] = channels[n][i];
          meanValue += channels[n][i];
        }
        meanValues[i] = meanValue / channels.length;
      }
      return { float32Array, meanValues };
    }
  }

  /**
   * Converts 32-bit float data to 16-bit integers
   */
  floatTo16BitPCM(float32Array) {
    const buffer = new ArrayBuffer(float32Array.length * 2);
    const view = new DataView(buffer);
    let offset = 0;
    for (let i = 0; i < float32Array.length; i++, offset += 2) {
      let s = Math.max(-1, Math.min(1, float32Array[i]));
      view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
    }
    return buffer;
  }

  /**
   * Retrieves the most recent amplitude values from the audio stream
   * @param {number} channel
   */
  getValues(channel = -1) {
    const channels = this.readChannelData(this.chunks, channel);
    const { meanValues } = this.formatAudioData(channels);
    return { meanValues, channels };
  }

  /**
   * Exports chunks as an audio/wav file
   */
  export() {
    const channels = this.readChannelData(this.chunks);
    const { float32Array, meanValues } = this.formatAudioData(channels);
    const audioData = this.floatTo16BitPCM(float32Array);
    return {
      meanValues: meanValues,
      audio: {
        bitsPerSample: 16,
        channels: channels,
        data: audioData,
      },
    };
  }

  receive(e) {
    const { event, id } = e.data;
    let receiptData = {};
    switch (event) {
      case 'start':
        this.recording = true;
        break;
      case 'stop':
        this.recording = false;
        break;
      case 'clear':
        this.initialize();
        break;
      case 'export':
        receiptData = this.export();
        break;
      case 'read':
        receiptData = this.getValues();
        break;
      default:
        break;
    }
    // Always send back receipt
    this.port.postMessage({ event: 'receipt', id, data: receiptData });
  }

  sendChunk(chunk) {
    const channels = this.readChannelData([chunk]);
    const { float32Array, meanValues } = this.formatAudioData(channels);
    const rawAudioData = this.floatTo16BitPCM(float32Array);
    const monoAudioData = this.floatTo16BitPCM(meanValues);
    this.port.postMessage({
      event: 'chunk',
      data: {
        mono: monoAudioData,
        raw: rawAudioData,
      },
    });
  }

  process(inputList, outputList, parameters) {
    // Copy input to output (e.g. speakers)
    // Note that this creates choppy sounds with Mac products
    const sourceLimit = Math.min(inputList.length, outputList.length);
    for (let inputNum = 0; inputNum < sourceLimit; inputNum++) {
      const input = inputList[inputNum];
      const output = outputList[inputNum];
      const channelCount = Math.min(input.length, output.length);
      for (let channelNum = 0; channelNum < channelCount; channelNum++) {
        input[channelNum].forEach((sample, i) => {
          output[channelNum][i] = sample;
        });
      }
    }
    const inputs = inputList[0];
    // There's latency at the beginning of a stream before recording starts
    // Make sure we actually receive audio data before we start storing chunks
    let sliceIndex = 0;
    if (!this.foundAudio) {
      for (const channel of inputs) {
        sliceIndex = 0; // reset for each channel
        if (this.foundAudio) {
          break;
        }
        if (channel) {
          for (const value of channel) {
            if (value !== 0) {
              // find only one non-zero entry in any channel
              this.foundAudio = true;
              break;
            } else {
              sliceIndex++;
            }
          }
        }
      }
    }
    if (inputs && inputs[0] && this.foundAudio && this.recording) {
      // We need to copy the TypedArray, because the \`process\`
      // internals will reuse the same buffer to hold each input
      const chunk = inputs.map((input) => input.slice(sliceIndex));
      this.chunks.push(chunk);
      this.sendChunk(chunk);
    }
    return true;
  }
}

registerProcessor('audio_processor', AudioProcessor);
`;
const audio_processor_script = new Blob([
    AudioProcessorWorklet
], {
    type: 'application/javascript'
});
const audio_processor_src = URL.createObjectURL(audio_processor_script);
const AudioProcessorSrc = audio_processor_src;
/**
 * Raw wav audio file contents
 * @typedef {Object} WavPackerAudioType
 * @property {Blob} blob
 * @property {string} url
 * @property {number} channelCount
 * @property {number} sampleRate
 * @property {number} duration
 */ /**
 * Utility class for assembling PCM16 "audio/wav" data
 * @class
 */ class WavPacker {
    /**
   * Converts Float32Array of amplitude data to ArrayBuffer in Int16Array format
   * @param {Float32Array} float32Array
   * @returns {ArrayBuffer}
   */ static floatTo16BitPCM(float32Array) {
        const buffer = new ArrayBuffer(2 * float32Array.length);
        const view = new DataView(buffer);
        let offset = 0;
        for(let i = 0; i < float32Array.length; i++, offset += 2){
            let s = Math.max(-1, Math.min(1, float32Array[i]));
            view.setInt16(offset, s < 0 ? 0x8000 * s : 0x7fff * s, true);
        }
        return buffer;
    }
    /**
   * Concatenates two ArrayBuffers
   * @param {ArrayBuffer} leftBuffer
   * @param {ArrayBuffer} rightBuffer
   * @returns {ArrayBuffer}
   */ static mergeBuffers(leftBuffer, rightBuffer) {
        const tmpArray = new Uint8Array(leftBuffer.byteLength + rightBuffer.byteLength);
        tmpArray.set(new Uint8Array(leftBuffer), 0);
        tmpArray.set(new Uint8Array(rightBuffer), leftBuffer.byteLength);
        return tmpArray.buffer;
    }
    /**
   * Packs data into an Int16 format
   * @private
   * @param {number} size 0 = 1x Int16, 1 = 2x Int16
   * @param {number} arg value to pack
   * @returns
   */ _packData(size, arg) {
        return [
            new Uint8Array([
                arg,
                arg >> 8
            ]),
            new Uint8Array([
                arg,
                arg >> 8,
                arg >> 16,
                arg >> 24
            ])
        ][size];
    }
    /**
   * Packs audio into "audio/wav" Blob
   * @param {number} sampleRate
   * @param {{bitsPerSample: number, channels: Array<Float32Array>, data: Int16Array}} audio
   * @returns {WavPackerAudioType}
   */ pack(sampleRate, audio) {
        if (null == audio ? void 0 : audio.bitsPerSample) {
            if (null == audio ? void 0 : audio.channels) {
                if (!(null == audio ? void 0 : audio.data)) throw new Error('Missing "data"');
            } else throw new Error('Missing "channels"');
        } else throw new Error('Missing "bitsPerSample"');
        const { bitsPerSample, channels, data } = audio;
        const output = [
            // Header
            'RIFF',
            this._packData(1, 52),
            'WAVE',
            // chunk 1
            'fmt ',
            this._packData(1, 16),
            this._packData(0, 1),
            this._packData(0, channels.length),
            this._packData(1, sampleRate),
            this._packData(1, sampleRate * channels.length * bitsPerSample / 8),
            this._packData(0, channels.length * bitsPerSample / 8),
            this._packData(0, bitsPerSample),
            // chunk 2
            'data',
            this._packData(1, channels[0].length * channels.length * bitsPerSample / 8),
            data
        ];
        const blob = new Blob(output, {
            type: 'audio/mpeg'
        });
        const url = URL.createObjectURL(blob);
        return {
            blob,
            url,
            channelCount: channels.length,
            sampleRate,
            duration: data.byteLength / (channels.length * sampleRate * 2)
        };
    }
}
globalThis.WavPacker = WavPacker;
/**
 * Decodes audio into a wav file
 * @typedef {Object} DecodedAudioType
 * @property {Blob} blob
 * @property {string} url
 * @property {Float32Array} values
 * @property {AudioBuffer} audioBuffer
 */ /**
 * Records live stream of user audio as PCM16 "audio/wav" data
 * @class
 */ class WavRecorder {
    /**
   * Decodes audio data from multiple formats to a Blob, url, Float32Array and AudioBuffer
   * @param {Blob|Float32Array|Int16Array|ArrayBuffer|number[]} audioData
   * @param {number} sampleRate
   * @param {number} fromSampleRate
   * @returns {Promise<DecodedAudioType>}
   */ static async decode(audioData) {
        let sampleRate = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : 44100, fromSampleRate = arguments.length > 2 && void 0 !== arguments[2] ? arguments[2] : -1;
        const context = new AudioContext({
            sampleRate
        });
        let arrayBuffer;
        let blob;
        if (audioData instanceof Blob) {
            if (-1 !== fromSampleRate) throw new Error('Can not specify "fromSampleRate" when reading from Blob');
            blob = audioData;
            arrayBuffer = await blob.arrayBuffer();
        } else if (audioData instanceof ArrayBuffer) {
            if (-1 !== fromSampleRate) throw new Error('Can not specify "fromSampleRate" when reading from ArrayBuffer');
            arrayBuffer = audioData;
            blob = new Blob([
                arrayBuffer
            ], {
                type: 'audio/wav'
            });
        } else {
            let float32Array;
            let data;
            if (audioData instanceof Int16Array) {
                data = audioData;
                float32Array = new Float32Array(audioData.length);
                for(let i = 0; i < audioData.length; i++)float32Array[i] = audioData[i] / 0x8000;
            } else if (audioData instanceof Float32Array) float32Array = audioData;
            else if (audioData instanceof Array) float32Array = new Float32Array(audioData);
            else throw new Error('"audioData" must be one of: Blob, Float32Arrray, Int16Array, ArrayBuffer, Array<number>');
            if (-1 === fromSampleRate) throw new Error('Must specify "fromSampleRate" when reading from Float32Array, In16Array or Array');
            if (fromSampleRate < 3000) throw new Error('Minimum "fromSampleRate" is 3000 (3kHz)');
            if (!data) data = WavPacker.floatTo16BitPCM(float32Array);
            const audio = {
                bitsPerSample: 16,
                channels: [
                    float32Array
                ],
                data
            };
            const packer = new WavPacker();
            const result = packer.pack(fromSampleRate, audio);
            blob = result.blob;
            arrayBuffer = await blob.arrayBuffer();
        }
        const audioBuffer = await context.decodeAudioData(arrayBuffer);
        const values = audioBuffer.getChannelData(0);
        const url = URL.createObjectURL(blob);
        return {
            blob,
            url,
            values,
            audioBuffer
        };
    }
    /**
   * Logs data in debug mode
   * @param {...any} arguments
   * @returns {true}
   */ log() {
        if (this.debug) this.log(...arguments);
        return true;
    }
    /**
   * Retrieves the current status of the recording
   * @returns {"ended"|"paused"|"recording"}
   */ getStatus() {
        if (!this.processor) return 'ended';
        if (!this.recording) return 'paused';
        return 'recording';
    }
    /**
   * Sends an event to the AudioWorklet
   * @private
   * @param {string} name
   * @param {{[key: string]: any}} data
   * @param {AudioWorkletNode} [_processor]
   * @returns {Promise<{[key: string]: any}>}
   */ async _event(name) {
        let data = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : {}, _processor = arguments.length > 2 && void 0 !== arguments[2] ? arguments[2] : null;
        _processor = _processor || this.processor;
        if (!_processor) throw new Error('Can not send events without recording first');
        const message = {
            event: name,
            id: this._lastEventId++,
            data
        };
        _processor.port.postMessage(message);
        const t0 = new Date().valueOf();
        while(!this.eventReceipts[message.id]){
            if (new Date().valueOf() - t0 > this.eventTimeout) throw new Error(`Timeout waiting for "${name}" event`);
            await new Promise((res)=>setTimeout(()=>res(true), 1));
        }
        const payload = this.eventReceipts[message.id];
        delete this.eventReceipts[message.id];
        return payload;
    }
    /**
   * Sets device change callback, remove if callback provided is `null`
   * @param {(Array<MediaDeviceInfo & {default: boolean}>): void|null} callback
   * @returns {true}
   */ listenForDeviceChange(callback) {
        if (null === callback && this._deviceChangeCallback) {
            navigator.mediaDevices.removeEventListener('devicechange', this._deviceChangeCallback);
            this._deviceChangeCallback = null;
        } else if (null !== callback) {
            // Basically a debounce; we only want this called once when devices change
            // And we only want the most recent callback() to be executed
            // if a few are operating at the same time
            let lastId = 0;
            let lastDevices = [];
            const serializeDevices = (devices)=>devices.map((d)=>d.deviceId).sort().join(',');
            const cb = async ()=>{
                let id = ++lastId;
                const devices = await this.listDevices();
                if (id === lastId) {
                    if (serializeDevices(lastDevices) !== serializeDevices(devices)) {
                        lastDevices = devices;
                        callback(devices.slice());
                    }
                }
            };
            navigator.mediaDevices.addEventListener('devicechange', cb);
            cb();
            this._deviceChangeCallback = cb;
        }
        return true;
    }
    /**
   * Manually request permission to use the microphone
   * @returns {Promise<true>}
   */ async requestPermission() {
        const permissionStatus = await navigator.permissions.query({
            name: 'microphone'
        });
        if ('denied' === permissionStatus.state) window.alert('You must grant microphone access to use this feature.');
        else if ('prompt' === permissionStatus.state) try {
            const stream = await navigator.mediaDevices.getUserMedia({
                audio: true
            });
            const tracks = stream.getTracks();
            tracks.forEach((track)=>track.stop());
        } catch (e) {
            window.alert('You must grant microphone access to use this feature.');
        }
        return true;
    }
    async getSampleRate() {
        return this.sampleRate;
    // console.log('[wav_recorder] getSampleRate', this.stream.getAudioTracks());
    // return this.stream.getAudioTracks()[0].getSettings().sampleRate;
    }
    /**
   * List all eligible devices for recording, will request permission to use microphone
   * @returns {Promise<Array<MediaDeviceInfo & {default: boolean}>>}
   */ async listDevices() {
        if (!navigator.mediaDevices || !('enumerateDevices' in navigator.mediaDevices)) throw new Error('Could not request user devices');
        await this.requestPermission();
        const devices = await navigator.mediaDevices.enumerateDevices();
        const audioDevices = devices.filter((device)=>'audioinput' === device.kind);
        const defaultDeviceIndex = audioDevices.findIndex((device)=>'default' === device.deviceId);
        const deviceList = [];
        if (-1 !== defaultDeviceIndex) {
            let defaultDevice = audioDevices.splice(defaultDeviceIndex, 1)[0];
            let existingIndex = audioDevices.findIndex((device)=>device.groupId === defaultDevice.groupId);
            if (-1 !== existingIndex) defaultDevice = audioDevices.splice(existingIndex, 1)[0];
            defaultDevice.default = true;
            deviceList.push(defaultDevice);
        }
        return deviceList.concat(audioDevices);
    }
    /**
   * Begins a recording session and requests microphone permissions if not already granted
   * Microphone recording indicator will appear on browser tab but status will be "paused"
   * @param {string} [deviceId] if no device provided, default device will be used
   * @returns {Promise<true>}
   */ async begin(param) {
        let { deviceId, audioTrackConfig } = param;
        if (this.processor) throw new Error("Already connected: please call .end() to start a new session");
        if (!navigator.mediaDevices || !('getUserMedia' in navigator.mediaDevices)) throw new Error('Could not request user media');
        try {
            const audioConstraints = {
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true
            };
            if (deviceId) audioConstraints.deviceId = {
                exact: deviceId
            };
            const config = {
                audio: audioConstraints
            };
            this.stream = await navigator.mediaDevices.getUserMedia(config);
        } catch (err) {
            throw new Error('Could not start media stream');
        }
        const context = new AudioContext({
            sampleRate: this.sampleRate
        });
        const source = context.createMediaStreamSource(this.stream);
        // Load and execute the module script.
        try {
            await context.audioWorklet.addModule(this.scriptSrc);
        } catch (e) {
            console.error(e);
            throw new Error(`Could not add audioWorklet module: ${this.scriptSrc}`);
        }
        const processor = new AudioWorkletNode(context, 'audio_processor');
        processor.port.onmessage = (e)=>{
            const { event, id, data } = e.data;
            if ('receipt' === event) this.eventReceipts[id] = data;
            else if ('chunk' === event) {
                if (this._chunkProcessorSize) {
                    const buffer = this._chunkProcessorBuffer;
                    this._chunkProcessorBuffer = {
                        raw: WavPacker.mergeBuffers(buffer.raw, data.raw),
                        mono: WavPacker.mergeBuffers(buffer.mono, data.mono)
                    };
                    if (this._chunkProcessorBuffer.mono.byteLength >= this._chunkProcessorSize) {
                        this._chunkProcessor(this._chunkProcessorBuffer);
                        this._chunkProcessorBuffer = {
                            raw: new ArrayBuffer(0),
                            mono: new ArrayBuffer(0)
                        };
                    }
                } else this._chunkProcessor(data);
            }
        };
        const node = source.connect(processor);
        const analyser = context.createAnalyser();
        analyser.fftSize = 8192;
        analyser.smoothingTimeConstant = 0.1;
        node.connect(analyser);
        if (this.outputToSpeakers) {
            // eslint-disable-next-line no-console
            console.warn("Warning: Output to speakers may affect sound quality,\nespecially due to system audio feedback preventative measures.\nuse only for debugging");
            analyser.connect(context.destination);
        }
        this.source = source;
        this.node = node;
        this.analyser = analyser;
        this.processor = processor;
        return true;
    }
    /**
   * Gets the current frequency domain data from the recording track
   * @param {"frequency"|"music"|"voice"} [analysisType]
   * @param {number} [minDecibels] default -100
   * @param {number} [maxDecibels] default -30
   * @returns {import('./analysis/audio_analysis.js').AudioAnalysisOutputType}
   */ getFrequencies() {
        let analysisType = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : 'frequency', minDecibels = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : -100, maxDecibels = arguments.length > 2 && void 0 !== arguments[2] ? arguments[2] : -30;
        if (!this.processor) throw new Error('Session ended: please call .begin() first');
        return AudioAnalysis.getFrequencies(this.analyser, this.sampleRate, null, analysisType, minDecibels, maxDecibels);
    }
    /**
   * Pauses the recording
   * Keeps microphone stream open but halts storage of audio
   * @returns {Promise<true>}
   */ async pause() {
        if (this.processor) {
            if (!this.recording) throw new Error('Already paused: please call .record() first');
        } else throw new Error('Session ended: please call .begin() first');
        if (this._chunkProcessorBuffer.raw.byteLength) this._chunkProcessor(this._chunkProcessorBuffer);
        this.log('Pausing ...');
        await this._event('stop');
        this.recording = false;
        return true;
    }
    /**
   * Start recording stream and storing to memory from the connected audio source
   * @param {(data: { mono: Int16Array; raw: Int16Array }) => any} [chunkProcessor]
   * @param {number} [chunkSize] chunkProcessor will not be triggered until this size threshold met in mono audio
   * @returns {Promise<true>}
   */ async record() {
        let chunkProcessor = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : ()=>{}, chunkSize = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : 8192;
        if (this.processor) {
            if (this.recording) throw new Error('Already recording: please call .pause() first');
            else if ('function' != typeof chunkProcessor) throw new Error("chunkProcessor must be a function");
        } else throw new Error('Session ended: please call .begin() first');
        this._chunkProcessor = chunkProcessor;
        this._chunkProcessorSize = chunkSize;
        this._chunkProcessorBuffer = {
            raw: new ArrayBuffer(0),
            mono: new ArrayBuffer(0)
        };
        this.log('Recording ...');
        await this._event('start');
        this.recording = true;
        return true;
    }
    /**
   * Clears the audio buffer, empties stored recording
   * @returns {Promise<true>}
   */ async clear() {
        if (!this.processor) throw new Error('Session ended: please call .begin() first');
        await this._event('clear');
        return true;
    }
    /**
   * Reads the current audio stream data
   * @returns {Promise<{meanValues: Float32Array, channels: Array<Float32Array>}>}
   */ async read() {
        if (!this.processor) throw new Error('Session ended: please call .begin() first');
        this.log('Reading ...');
        const result = await this._event('read');
        return result;
    }
    /**
   * Saves the current audio stream to a file
   * @param {boolean} [force] Force saving while still recording
   * @returns {Promise<import('./wav_packer.js').WavPackerAudioType>}
   */ async save() {
        let force = arguments.length > 0 && void 0 !== arguments[0] && arguments[0];
        if (!this.processor) throw new Error('Session ended: please call .begin() first');
        if (!force && this.recording) throw new Error('Currently recording: please call .pause() first, or call .save(true) to force');
        this.log('Exporting ...');
        const exportData = await this._event('export');
        const packer = new WavPacker();
        const result = packer.pack(this.sampleRate, exportData.audio);
        return result;
    }
    /**
   * Ends the current recording session and saves the result
   * @returns {Promise<import('./wav_packer.js').WavPackerAudioType>}
   */ async end() {
        if (!this.processor) throw new Error('Session ended: please call .begin() first');
        const _processor = this.processor;
        this.log('Stopping ...');
        await this._event('stop');
        this.recording = false;
        const tracks = this.stream.getTracks();
        tracks.forEach((track)=>track.stop());
        this.log('Exporting ...');
        const exportData = await this._event('export', {}, _processor);
        this.processor.disconnect();
        this.source.disconnect();
        this.node.disconnect();
        this.analyser.disconnect();
        this.stream = null;
        this.processor = null;
        this.source = null;
        this.node = null;
        const packer = new WavPacker();
        const result = packer.pack(this.sampleRate, exportData.audio);
        return result;
    }
    /**
   * Performs a full cleanup of WavRecorder instance
   * Stops actively listening via microphone and removes existing listeners
   * @returns {Promise<true>}
   */ async quit() {
        this.listenForDeviceChange(null);
        if (this.processor) await this.end();
        return true;
    }
    /**
   * Create a new WavRecorder instance
   * @param {{sampleRate?: number, outputToSpeakers?: boolean, debug?: boolean}} [options]
   * @returns {WavRecorder}
   */ constructor({ sampleRate = 44100, outputToSpeakers = false, debug = false } = {}){
        // Script source
        this.scriptSrc = AudioProcessorSrc;
        // Config
        this.sampleRate = sampleRate;
        this.outputToSpeakers = outputToSpeakers;
        this.debug = !!debug;
        this._deviceChangeCallback = null;
        this._devices = [];
        // State variables
        this.stream = null;
        this.processor = null;
        this.source = null;
        this.node = null;
        this.recording = false;
        // Event handling with AudioWorklet
        this._lastEventId = 0;
        this.eventReceipts = {};
        this.eventTimeout = 5000;
        // Process chunks of audio
        this._chunkProcessor = ()=>{};
        this._chunkProcessorSize = void 0;
        this._chunkProcessorBuffer = {
            raw: new ArrayBuffer(0),
            mono: new ArrayBuffer(0)
        };
    }
}
globalThis.WavRecorder = WavRecorder;
class APIResource {
    constructor(client){
        this._client = client;
    }
}
/* eslint-disable @typescript-eslint/no-namespace */ class Bots extends APIResource {
    /**
   * Create a new agent. | 调用接口创建一个新的智能体。
   * @docs en:https://www.coze.com/docs/developer_guides/create_bot?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/create_bot?_lang=zh
   * @param params - Required The parameters for creating a bot. | 创建 Bot 的参数。
   * @param params.space_id - Required The Space ID of the space where the agent is located. | Bot 所在的空间的 Space ID。
   * @param params.name - Required The name of the agent. It should be 1 to 20 characters long. | Bot 的名称。
   * @param params.description - Optional The description of the agent. It can be 0 to 500 characters long. | Bot 的描述信息。
   * @param params.icon_file_id - Optional The file ID for the agent's avatar. | 作为智能体头像的文件 ID。
   * @param params.prompt_info - Optional The personality and reply logic of the agent. | Bot 的提示词配置。
   * @param params.onboarding_info - Optional The settings related to the agent's opening remarks. | Bot 的开场白配置。
   * @returns Information about the created bot. | 创建的 Bot 信息。
   */ async create(params, options) {
        const apiUrl = '/v1/bot/create';
        const result = await this._client.post(apiUrl, params, false, options);
        return result.data;
    }
    /**
   * Update the configuration of an agent. | 调用接口修改智能体的配置。
   * @docs en:https://www.coze.com/docs/developer_guides/update_bot?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/update_bot?_lang=zh
   * @param params - Required The parameters for updating a bot. | 修改 Bot 的参数。
   * @param params.bot_id - Required The ID of the agent that the API interacts with. | 待修改配置的智能体ID。
   * @param params.name - Optional The name of the agent. | Bot 的名称。
   * @param params.description - Optional The description of the agent. | Bot 的描述信息。
   * @param params.icon_file_id - Optional The file ID for the agent's avatar. | 作为智能体头像的文件 ID。
   * @param params.prompt_info - Optional The personality and reply logic of the agent. | Bot 的提示词配置。
   * @param params.onboarding_info - Optional The settings related to the agent's opening remarks. | Bot 的开场白配置。
   * @param params.knowledge - Optional Knowledge configurations of the agent. | Bot 的知识库配置。
   * @returns Undefined | 无返回值
   */ async update(params, options) {
        const apiUrl = '/v1/bot/update';
        const result = await this._client.post(apiUrl, params, false, options);
        return result.data;
    }
    /**
   * Get the agents published as API service. | 调用接口查看指定空间发布到 Agent as API 渠道的智能体列表。
   * @docs en:https://www.coze.com/docs/developer_guides/published_bots_list?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/published_bots_list?_lang=zh
   * @param params - Required The parameters for listing bots. | 列出 Bot 的参数。
   * @param params.space_id - Required The ID of the space. | Bot 所在的空间的 Space ID。
   * @param params.page_size - Optional Pagination size. | 分页大小。
   * @param params.page_index - Optional Page number for paginated queries. | 分页查询时的页码。
   * @returns List of published bots. | 已发布的 Bot 列表。
   */ async list(params, options) {
        const apiUrl = '/v1/space/published_bots_list';
        const result = await this._client.get(apiUrl, params, false, options);
        return result.data;
    }
    /**
   * Publish the specified agent as an API service. | 调用接口创建一个新的智能体。
   * @docs en:https://www.coze.com/docs/developer_guides/publish_bot?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/publish_bot?_lang=zh
   * @param params - Required The parameters for publishing a bot. | 发布 Bot 的参数。
   * @param params.bot_id - Required The ID of the agent that the API interacts with. | 要发布的智能体ID。
   * @param params.connector_ids - Required The list of publishing channel IDs for the agent. | 智能体的发布渠道 ID 列表。
   * @returns Undefined | 无返回值
   */ async publish(params, options) {
        const apiUrl = '/v1/bot/publish';
        const result = await this._client.post(apiUrl, params, false, options);
        return result.data;
    }
    /**
   * Get the configuration information of the agent. | 获取指定智能体的配置信息。
   * @docs en:https://www.coze.com/docs/developer_guides/get_metadata?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/get_metadata?_lang=zh
   * @param params - Required The parameters for retrieving a bot. | 获取 Bot 的参数。
   * @param params.bot_id - Required The ID of the agent that the API interacts with. | 要查看的智能体ID。
   * @returns Information about the bot. | Bot 的配置信息。
   */ async retrieve(params, options) {
        const apiUrl = '/v1/bot/get_online_info';
        const result = await this._client.get(apiUrl, params, false, options);
        return result.data;
    }
}
/* eslint-disable security/detect-object-injection */ /* eslint-disable @typescript-eslint/no-explicit-any */ function safeJsonParse(jsonString) {
    let defaultValue = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : '';
    try {
        return JSON.parse(jsonString);
    } catch (error) {
        return defaultValue;
    }
}
function utils_sleep(ms) {
    return new Promise((resolve)=>{
        setTimeout(resolve, ms);
    });
}
function utils_isUniApp() {
    return 'undefined' != typeof uni;
}
function utils_isBrowser() {
    return 'undefined' != typeof window;
}
function isPlainObject(obj) {
    if ('object' != typeof obj || null === obj) return false;
    const proto = Object.getPrototypeOf(obj);
    if (null === proto) return true;
    let baseProto = proto;
    while(null !== Object.getPrototypeOf(baseProto))baseProto = Object.getPrototypeOf(baseProto);
    return proto === baseProto;
}
function mergeConfig() {
    for(var _len = arguments.length, objects = new Array(_len), _key = 0; _key < _len; _key++)objects[_key] = arguments[_key];
    return objects.reduce((result, obj)=>{
        if (void 0 === obj) return result || {};
        for(const key in obj)if (Object.prototype.hasOwnProperty.call(obj, key)) {
            if (isPlainObject(obj[key]) && !Array.isArray(obj[key])) result[key] = mergeConfig(result[key] || {}, obj[key]);
            else result[key] = obj[key];
        }
        return result;
    }, {});
}
function isPersonalAccessToken(token) {
    return null == token ? void 0 : token.startsWith('pat_');
}
function buildWebsocketUrl(path, params) {
    const queryString = Object.entries(params).filter((param)=>{
        let [_, value] = param;
        return null != value && '' !== value;
    }).map((param)=>{
        let [key, value] = param;
        return `${key}=${value}`;
    }).join('&');
    return `${path}?${queryString}`;
}
/* eslint-disable max-params */ class CozeError extends Error {
}
class error_APIError extends CozeError {
    static makeMessage(status, errorBody, message, headers) {
        if (!errorBody && message) return message;
        if (errorBody) {
            const list = [];
            const { code, msg, error } = errorBody;
            if (code) list.push(`code: ${code}`);
            if (msg) list.push(`msg: ${msg}`);
            if ((null == error ? void 0 : error.detail) && msg !== error.detail) list.push(`detail: ${error.detail}`);
            const logId = (null == error ? void 0 : error.logid) || (null == headers ? void 0 : headers['x-tt-logid']);
            if (logId) list.push(`logid: ${logId}`);
            return list.join(', ');
        }
        if (status) return `http status code: ${status} (no body)`;
        return '(no status code or body)';
    }
    static generate(status, errorResponse, message, headers) {
        if (!status) return new APIConnectionError({
            cause: castToError(errorResponse)
        });
        const error = errorResponse;
        // https://www.coze.cn/docs/developer_guides/coze_error_codes
        if (400 === status || (null == error ? void 0 : error.code) === 4000) return new BadRequestError(status, error, message, headers);
        if (401 === status || (null == error ? void 0 : error.code) === 4100) return new AuthenticationError(status, error, message, headers);
        if (403 === status || (null == error ? void 0 : error.code) === 4101) return new PermissionDeniedError(status, error, message, headers);
        if (404 === status || (null == error ? void 0 : error.code) === 4200) return new NotFoundError(status, error, message, headers);
        if (429 === status || (null == error ? void 0 : error.code) === 4013) return new RateLimitError(status, error, message, headers);
        if (408 === status) return new TimeoutError(status, error, message, headers);
        if (502 === status) return new GatewayError(status, error, message, headers);
        if (status >= 500) return new InternalServerError(status, error, message, headers);
        return new error_APIError(status, error, message, headers);
    }
    constructor(status, error, message, headers){
        var _error_detail, _error_error;
        super(`${error_APIError.makeMessage(status, error, message, headers)}`);
        this.status = status;
        this.headers = headers;
        this.logid = (null == error ? void 0 : null === (_error_detail = error.detail) || void 0 === _error_detail ? void 0 : _error_detail.logid) || (null == headers ? void 0 : headers['x-tt-logid']);
        // this.error = error;
        this.code = null == error ? void 0 : error.code;
        this.msg = null == error ? void 0 : error.msg;
        this.detail = null == error ? void 0 : null === (_error_error = error.error) || void 0 === _error_error ? void 0 : _error_error.detail;
        this.rawError = error;
    }
}
class APIConnectionError extends error_APIError {
    constructor({ message }){
        super(void 0, void 0, message || 'Connection error.', void 0), this.status = void 0;
    // if (cause) {
    //   this.cause = cause;
    // }
    }
}
class APIUserAbortError extends error_APIError {
    constructor(message){
        super(void 0, void 0, message || 'Request was aborted.', void 0), this.name = 'UserAbortError', this.status = void 0;
    }
}
class BadRequestError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'BadRequestError', this.status = 400;
    }
}
class AuthenticationError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'AuthenticationError', this.status = 401;
    }
}
class PermissionDeniedError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'PermissionDeniedError', this.status = 403;
    }
}
class NotFoundError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'NotFoundError', this.status = 404;
    }
}
class TimeoutError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'TimeoutError', this.status = 408;
    }
}
class RateLimitError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'RateLimitError', this.status = 429;
    }
}
class InternalServerError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'InternalServerError', this.status = 500;
    }
}
class GatewayError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'GatewayError', this.status = 502;
    }
}
const castToError = (err)=>{
    if (err instanceof Error) return err;
    return new Error(err);
};
class Messages extends APIResource {
    /**
   * Get the list of messages in a chat. | 获取对话中的消息列表。
   * @docs en:https://www.coze.com/docs/developer_guides/chat_message_list?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/chat_message_list?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | 会话 ID。
   * @param chat_id - Required The ID of the chat. | 对话 ID。
   * @returns An array of chat messages. | 对话消息数组。
   */ async list(conversation_id, chat_id, options) {
        const apiUrl = `/v3/chat/message/list?conversation_id=${conversation_id}&chat_id=${chat_id}`;
        const result = await this._client.get(apiUrl, void 0, false, options);
        return result.data;
    }
}
const uuid = ()=>(Math.random() * new Date().getTime()).toString();
const handleAdditionalMessages = (additional_messages)=>null == additional_messages ? void 0 : additional_messages.map((i)=>({
            ...i,
            content: 'object' == typeof i.content ? JSON.stringify(i.content) : i.content
        }));
const handleParameters = (parameters)=>{
    if (parameters) {
        for (const [key, value] of Object.entries(parameters))if ('object' == typeof value) parameters[key] = JSON.stringify(value);
    }
    return parameters;
};
class Chat extends APIResource {
    /**
   * Call the Chat API to send messages to a published Coze agent. | 调用此接口发起一次对话，支持添加上下文
   * @docs en:https://www.coze.com/docs/developer_guides/chat_v3?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/chat_v3?_lang=zh
   * @param params - Required The parameters for creating a chat session. | 创建会话的参数。
   * @param params.bot_id - Required The ID of the agent. | 要进行会话聊天的 Bot ID。
   * @param params.user_id - Optional The ID of the user interacting with the Bot. | 标识当前与 Bot 交互的用户。
   * @param params.additional_messages - Optional Additional messages for the conversation. | 对话的附加信息。
   * @param params.custom_variables - Optional Variables defined in the Bot. | Bot 中定义变量。
   * @param params.auto_save_history - Optional Whether to automatically save the conversation history. | 是否自动保存历史对话记录。
   * @param params.meta_data - Optional Additional metadata for the message. | 创建消息时的附加消息。
   * @param params.conversation_id - Optional The ID of the conversation. | 标识对话发生在哪一次会话中。
   * @param params.extra_params - Optional Extra parameters for the conversation. | 附加参数。
   * @returns The data of the created chat. | 创建的对话数据。
   */ async create(params, options) {
        if (!params.user_id) params.user_id = uuid();
        const { conversation_id, ...rest } = params;
        const apiUrl = `/v3/chat${conversation_id ? `?conversation_id=${conversation_id}` : ''}`;
        const payload = {
            ...rest,
            additional_messages: handleAdditionalMessages(params.additional_messages),
            shortcut_command: params.shortcut_command ? {
                ...params.shortcut_command,
                parameters: handleParameters(params.shortcut_command.parameters)
            } : void 0,
            stream: false
        };
        const result = await this._client.post(apiUrl, payload, false, options);
        return result.data;
    }
    /**
   * Call the Chat API to send messages to a published Coze agent. | 调用此接口发起一次对话，支持添加上下文
   * @docs en:https://www.coze.com/docs/developer_guides/chat_v3?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/chat_v3?_lang=zh
   * @param params - Required The parameters for creating a chat session. | 创建会话的参数。
   * @param params.bot_id - Required The ID of the agent. | 要进行会话聊天的 Bot ID。
   * @param params.user_id - Optional The ID of the user interacting with the Bot. | 标识当前与 Bot 交互的用户。
   * @param params.additional_messages - Optional Additional messages for the conversation. | 对话的附加信息。
   * @param params.custom_variables - Optional Variables defined in the Bot. | Bot 中定义的变量。
   * @param params.auto_save_history - Optional Whether to automatically save the conversation history. | 是否自动保存历史对话记录。
   * @param params.meta_data - Optional Additional metadata for the message. | 创建消息时的附加消息。
   * @param params.conversation_id - Optional The ID of the conversation. | 标识对话发生在哪一次会话中。
   * @param params.extra_params - Optional Extra parameters for the conversation. | 附加参数。
   * @returns
   */ async createAndPoll(params, options) {
        if (!params.user_id) params.user_id = uuid();
        const { conversation_id, ...rest } = params;
        const apiUrl = `/v3/chat${conversation_id ? `?conversation_id=${conversation_id}` : ''}`;
        const payload = {
            ...rest,
            additional_messages: handleAdditionalMessages(params.additional_messages),
            shortcut_command: params.shortcut_command ? {
                ...params.shortcut_command,
                parameters: handleParameters(params.shortcut_command.parameters)
            } : void 0,
            stream: false
        };
        const result = await this._client.post(apiUrl, payload, false, options);
        const chatId = result.data.id;
        const conversationId = result.data.conversation_id;
        let chat;
        while(true){
            await utils_sleep(100);
            chat = await this.retrieve(conversationId, chatId);
            if ('completed' === chat.status || 'failed' === chat.status || 'requires_action' === chat.status) break;
        }
        const messageList = await this.messages.list(conversationId, chatId);
        return {
            chat,
            messages: messageList
        };
    }
    /**
   * Call the Chat API to send messages to a published Coze agent with streaming response. | 调用此接口发起一次对话，支持流式响应。
   * @docs en:https://www.coze.com/docs/developer_guides/chat_v3?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/chat_v3?_lang=zh
   * @param params - Required The parameters for streaming a chat session. | 流式会话的参数。
   * @param params.bot_id - Required The ID of the agent. | 要进行会话聊天的 Bot ID。
   * @param params.user_id - Optional The ID of the user interacting with the Bot. | 标识当前与 Bot 交互的用户。
   * @param params.additional_messages - Optional Additional messages for the conversation. | 对话的附加信息。
   * @param params.custom_variables - Optional Variables defined in the Bot. | Bot 中定义的变量。
   * @param params.auto_save_history - Optional Whether to automatically save the conversation history. | 是否自动保存历史对话记录。
   * @param params.meta_data - Optional Additional metadata for the message. | 创建消息时的附加消息。
   * @param params.conversation_id - Optional The ID of the conversation. | 标识对话发生在哪一次会话中。
   * @param params.extra_params - Optional Extra parameters for the conversation. | 附加参数。
   * @returns A stream of chat data. | 对话数据流。
   */ async *stream(params, options) {
        if (!params.user_id) params.user_id = uuid();
        const { conversation_id, ...rest } = params;
        const apiUrl = `/v3/chat${conversation_id ? `?conversation_id=${conversation_id}` : ''}`;
        const payload = {
            ...rest,
            additional_messages: handleAdditionalMessages(params.additional_messages),
            shortcut_command: params.shortcut_command ? {
                ...params.shortcut_command,
                parameters: handleParameters(params.shortcut_command.parameters)
            } : void 0,
            stream: true
        };
        const result = await this._client.post(apiUrl, payload, true, options);
        for await (const message of result)if ("done" === message.event) {
            const ret = {
                event: message.event,
                data: '[DONE]'
            };
            yield ret;
        } else try {
            const ret = {
                event: message.event,
                data: JSON.parse(message.data)
            };
            yield ret;
        } catch (error) {
            throw new CozeError(`Could not parse message into JSON:${message.data}`);
        }
    }
    /**
   * Get the detailed information of the chat. | 查看对话的详细信息。
   * @docs en:https://www.coze.com/docs/developer_guides/retrieve_chat?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/retrieve_chat?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | 会话 ID。
   * @param chat_id - Required The ID of the chat. | 对话 ID。
   * @returns The data of the retrieved chat. | 检索到的对话数据。
   */ async retrieve(conversation_id, chat_id, options) {
        const apiUrl = `/v3/chat/retrieve?conversation_id=${conversation_id}&chat_id=${chat_id}`;
        const result = await this._client.post(apiUrl, void 0, false, options);
        return result.data;
    }
    /**
   * Cancel a chat session. | 取消对话会话。
   * @docs en:https://www.coze.com/docs/developer_guides/cancel_chat?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/cancel_chat?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | 会话 ID。
   * @param chat_id - Required The ID of the chat. | 对话 ID。
   * @returns The data of the canceled chat. | 取消的对话数据。
   */ async cancel(conversation_id, chat_id, options) {
        const apiUrl = '/v3/chat/cancel';
        const payload = {
            conversation_id,
            chat_id
        };
        const result = await this._client.post(apiUrl, payload, false, options);
        return result.data;
    }
    /**
   * Submit tool outputs for a chat session. | 提交对话会话的工具输出。
   * @docs en:https://www.coze.com/docs/developer_guides/chat_submit_tool_outputs?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/chat_submit_tool_outputs?_lang=zh
   * @param params - Required Parameters for submitting tool outputs. | 提交工具输出的参数。
   * @param params.conversation_id - Required The ID of the conversation. | 会话 ID。
   * @param params.chat_id - Required The ID of the chat. | 对话 ID。
   * @param params.tool_outputs - Required The outputs of the tool. | 工具的输出。
   * @param params.stream - Optional Whether to use streaming response. | 是否使用流式响应。
   * @returns The data of the submitted tool outputs or a stream of chat data. | 提交的工具输出数据或对话数据流。
   */ async *submitToolOutputs(params, options) {
        const { conversation_id, chat_id, ...rest } = params;
        const apiUrl = `/v3/chat/submit_tool_outputs?conversation_id=${params.conversation_id}&chat_id=${params.chat_id}`;
        const payload = {
            ...rest
        };
        if (false === params.stream) {
            const response = await this._client.post(apiUrl, payload, false, options);
            return response.data;
        }
        {
            const result = await this._client.post(apiUrl, payload, true, options);
            for await (const message of result)if ("done" === message.event) {
                const ret = {
                    event: message.event,
                    data: '[DONE]'
                };
                yield ret;
            } else try {
                const ret = {
                    event: message.event,
                    data: JSON.parse(message.data)
                };
                yield ret;
            } catch (error) {
                throw new CozeError(`Could not parse message into JSON:${message.data}`);
            }
        }
    }
    constructor(...args){
        super(...args), this.messages = new Messages(this._client);
    }
}
var chat_ChatEventType = /*#__PURE__*/ function(ChatEventType) {
    ChatEventType["CONVERSATION_CHAT_CREATED"] = "conversation.chat.created";
    ChatEventType["CONVERSATION_CHAT_IN_PROGRESS"] = "conversation.chat.in_progress";
    ChatEventType["CONVERSATION_CHAT_COMPLETED"] = "conversation.chat.completed";
    ChatEventType["CONVERSATION_CHAT_FAILED"] = "conversation.chat.failed";
    ChatEventType["CONVERSATION_CHAT_REQUIRES_ACTION"] = "conversation.chat.requires_action";
    ChatEventType["CONVERSATION_MESSAGE_DELTA"] = "conversation.message.delta";
    ChatEventType["CONVERSATION_MESSAGE_COMPLETED"] = "conversation.message.completed";
    ChatEventType["CONVERSATION_AUDIO_DELTA"] = "conversation.audio.delta";
    ChatEventType["DONE"] = "done";
    ChatEventType["ERROR"] = "error";
    return ChatEventType;
}({});
var chat_RoleType = /*#__PURE__*/ function(RoleType) {
    RoleType["User"] = "user";
    RoleType["Assistant"] = "assistant";
    return RoleType;
}({});
class messages_Messages extends APIResource {
    /**
   * Create a message and add it to the specified conversation. | 创建一条消息，并将其添加到指定的会话中。
   * @docs en: https://www.coze.com/docs/developer_guides/create_message?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/create_message?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @param params - Required The parameters for creating a message | 创建消息所需的参数
   * @param params.role - Required The entity that sent this message. Possible values: user, assistant. | 发送这条消息的实体。取值：user, assistant。
   * @param params.content - Required The content of the message. | 消息的内容。
   * @param params.content_type - Required The type of the message content. | 消息内容的类型。
   * @param params.meta_data - Optional Additional information when creating a message. | 创建消息时的附加消息。
   * @returns Information about the new message. | 消息详情。
   */ async create(conversation_id, params, options) {
        const apiUrl = `/v1/conversation/message/create?conversation_id=${conversation_id}`;
        const response = await this._client.post(apiUrl, params, false, options);
        return response.data;
    }
    /**
   * Modify a message, supporting the modification of message content, additional content, and message type. | 修改一条消息，支持修改消息内容、附加内容和消息类型。
   * @docs en: https://www.coze.com/docs/developer_guides/modify_message?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/modify_message?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @param message_id - Required The ID of the message. | Message ID，即消息的唯一标识。
   * @param params - Required The parameters for modifying a message | 修改消息所需的参数
   * @param params.meta_data - Optional Additional information when modifying a message. | 修改消息时的附加消息。
   * @param params.content - Optional The content of the message. | 消息的内容。
   * @param params.content_type - Optional The type of the message content. | 消息内容的类型。
   * @returns Information about the modified message. | 消息详情。
   */ // eslint-disable-next-line max-params
    async update(conversation_id, message_id, params, options) {
        const apiUrl = `/v1/conversation/message/modify?conversation_id=${conversation_id}&message_id=${message_id}`;
        const response = await this._client.post(apiUrl, params, false, options);
        return response.message;
    }
    /**
   * Get the detailed information of specified message. | 查看指定消息的详细信息。
   * @docs en: https://www.coze.com/docs/developer_guides/retrieve_message?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/retrieve_message?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @param message_id - Required The ID of the message. | Message ID，即消息的唯一标识。
   * @returns Information about the message. | 消息详情。
   */ async retrieve(conversation_id, message_id, options) {
        const apiUrl = `/v1/conversation/message/retrieve?conversation_id=${conversation_id}&message_id=${message_id}`;
        const response = await this._client.get(apiUrl, null, false, options);
        return response.data;
    }
    /**
   * List messages in a conversation. | 列出会话中的消息。
   * @docs en: https://www.coze.com/docs/developer_guides/message_list?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/message_list?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @param params - Optional The parameters for listing messages | 列出消息所需的参数
   * @param params.order - Optional The order of the messages. | 消息的顺序。
   * @param params.chat_id - Optional The ID of the chat. | 聊天 ID。
   * @param params.before_id - Optional The ID of the message before which to list. | 列出此消息之前的消息 ID。
   * @param params.after_id - Optional The ID of the message after which to list. | 列出此消息之后的消息 ID。
   * @param params.limit - Optional The maximum number of messages to return. | 返回的最大消息数。
   * @returns A list of messages. | 消息列表。
   */ async list(conversation_id, params, options) {
        const apiUrl = `/v1/conversation/message/list?conversation_id=${conversation_id}`;
        const response = await this._client.post(apiUrl, params, false, options);
        return response;
    }
    /**
   * Call the API to delete a message within a specified conversation. | 调用接口在指定会话中删除消息。
   * @docs en: https://www.coze.com/docs/developer_guides/delete_message?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/delete_message?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @param message_id - Required The ID of the message. | Message ID，即消息的唯一标识。
   * @returns Details of the deleted message. | 已删除的消息详情。
   */ async delete(conversation_id, message_id, options) {
        const apiUrl = `/v1/conversation/message/delete?conversation_id=${conversation_id}&message_id=${message_id}`;
        const response = await this._client.post(apiUrl, void 0, false, options);
        return response.data;
    }
}
class Conversations extends APIResource {
    /**
   * Create a conversation. Conversation is an interaction between an agent and a user, including one or more messages. | 调用接口创建一个会话。
   * @docs en: https://www.coze.com/docs/developer_guides/create_conversation?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/create_conversation?_lang=zh
   * @param params - Required The parameters for creating a conversation | 创建会话所需的参数
   * @param params.messages - Optional Messages in the conversation. | 会话中的消息内容。
   * @param params.meta_data - Optional Additional information when creating a message. | 创建消息时的附加消息。
   * @param params.bot_id - Optional Bind and isolate conversation on different bots. | 绑定和隔离不同Bot的会话。
   * @returns Information about the created conversation. | 会话的基础信息。
   */ async create(params, options) {
        const apiUrl = '/v1/conversation/create';
        const response = await this._client.post(apiUrl, params, false, options);
        return response.data;
    }
    /**
   * Get the information of specific conversation. | 通过会话 ID 查看会话信息。
   * @docs en: https://www.coze.com/docs/developer_guides/retrieve_conversation?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/retrieve_conversation?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @returns Information about the conversation. | 会话的基础信息。
   */ async retrieve(conversation_id, options) {
        const apiUrl = `/v1/conversation/retrieve?conversation_id=${conversation_id}`;
        const response = await this._client.get(apiUrl, null, false, options);
        return response.data;
    }
    /**
   * List all conversations. | 列出 Bot 下所有会话。
   * @param params
   * @param params.bot_id - Required Bot ID. | Bot ID。
   * @param params.page_num - Optional The page number. | 页码，默认值为 1。
   * @param params.page_size - Optional The number of conversations per page. | 每页的会话数量，默认值为 50。
   * @returns Information about the conversations. | 会话的信息。
   */ async list(params, options) {
        const apiUrl = '/v1/conversations';
        const response = await this._client.get(apiUrl, params, false, options);
        return response.data;
    }
    /**
   * Clear a conversation. | 清空会话。
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @returns Information about the conversation session. | 会话的会话 ID。
   */ async clear(conversation_id, options) {
        const apiUrl = `/v1/conversations/${conversation_id}/clear`;
        const response = await this._client.post(apiUrl, null, false, options);
        return response.data;
    }
    constructor(...args){
        super(...args), this.messages = new messages_Messages(this._client);
    }
}
const external_axios_namespaceObject = require("axios");
var external_axios_default = /*#__PURE__*/ __webpack_require__.n(external_axios_namespaceObject);
class Files extends APIResource {
    /**
   * Upload files to Coze platform. | 调用接口上传文件到扣子。
   * @docs en: https://www.coze.com/docs/developer_guides/upload_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/upload_files?_lang=zh
   * @param params - Required The parameters for file upload | 上传文件所需的参数
   * @param params.file - Required The file to be uploaded. | 需要上传的文件。
   * @returns Information about the new file. | 已上传的文件信息。
   */ async upload(params, options) {
        const apiUrl = '/v1/files/upload';
        const response = await this._client.post(apiUrl, (0, external_axios_namespaceObject.toFormData)(params), false, options);
        return response.data;
    }
    /**
   * Get the information of the specific file uploaded to Coze platform. | 查看已上传的文件详情。
   * @docs en: https://www.coze.com/docs/developer_guides/retrieve_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/retrieve_files?_lang=zh
   * @param file_id - Required The ID of the uploaded file. | 已上传的文件 ID。
   * @returns Information about the uploaded file. | 已上传的文件信息。
   */ async retrieve(file_id, options) {
        const apiUrl = `/v1/files/retrieve?file_id=${file_id}`;
        const response = await this._client.get(apiUrl, null, false, options);
        return response.data;
    }
}
class Runs extends APIResource {
    /**
   * Initiates a workflow run. | 启动工作流运行。
   * @docs en: https://www.coze.com/docs/developer_guides/workflow_run?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/workflow_run?_lang=zh
   * @param params.workflow_id - Required The ID of the workflow to run. | 必选 要运行的工作流 ID。
   * @param params.bot_id - Optional The ID of the bot associated with the workflow. | 可选 与工作流关联的机器人 ID。
   * @param params.parameters - Optional Parameters for the workflow execution. | 可选 工作流执行的参数。
   * @param params.ext - Optional Additional information for the workflow execution. | 可选 工作流执行的附加信息。
   * @param params.execute_mode - Optional The mode in which to execute the workflow. | 可选 工作流执行的模式。
   * @param params.connector_id - Optional The ID of the connector to use for the workflow. | 可选 用于工作流的连接器 ID。
   * @param params.app_id - Optional The ID of the app.  | 可选 要进行会话聊天的 App ID
   * @returns RunWorkflowData | 工作流运行数据
   */ async create(params, options) {
        const apiUrl = '/v1/workflow/run';
        const response = await this._client.post(apiUrl, params, false, options);
        return response;
    }
    /**
   * Streams the workflow run events. | 流式传输工作流运行事件。
   * @docs en: https://www.coze.com/docs/developer_guides/workflow_stream_run?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/workflow_stream_run?_lang=zh
   * @param params.workflow_id - Required The ID of the workflow to run. | 必选 要运行的工作流 ID。
   * @param params.bot_id - Optional The ID of the bot associated with the workflow. | 可选 与工作流关联的机器人 ID。
   * @param params.parameters - Optional Parameters for the workflow execution. | 可选 工作流执行的参数。
   * @param params.ext - Optional Additional information for the workflow execution. | 可选 工作流执行的附加信息。
   * @param params.execute_mode - Optional The mode in which to execute the workflow. | 可选 工作流执行的模式。
   * @param params.connector_id - Optional The ID of the connector to use for the workflow. | 可选 用于工作流的连接器 ID。
   * @param params.app_id - Optional The ID of the app.  | 可选 要进行会话聊天的 App ID
   * @returns Stream<WorkflowEvent, { id: string; event: string; data: string }> | 工作流事件流
   */ async *stream(params, options) {
        const apiUrl = '/v1/workflow/stream_run';
        const result = await this._client.post(apiUrl, params, true, options);
        for await (const message of result)try {
            if ("Done" === message.event) yield new WorkflowEvent(Number(message.id), "Done");
            else yield new WorkflowEvent(Number(message.id), message.event, JSON.parse(message.data));
        } catch (error) {
            throw new CozeError(`Could not parse message into JSON:${message.data}`);
        }
    }
    /**
   * Resumes a paused workflow run. | 恢复暂停的工作流运行。
   * @docs en: https://www.coze.com/docs/developer_guides/workflow_resume?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/workflow_resume?_lang=zh
   * @param params.workflow_id - Required The ID of the workflow to resume. | 必选 要恢复的工作流 ID。
   * @param params.event_id - Required The ID of the event to resume from. | 必选 要从中恢复的事件 ID。
   * @param params.resume_data - Required Data needed to resume the workflow. | 必选 恢复工作流所需的数据。
   * @param params.interrupt_type - Required The type of interruption to resume from. | 必选 要恢复的中断类型。
   * @returns { id: string; event: WorkflowEventType; data: WorkflowEventMessage | WorkflowEventInterrupt | WorkflowEventError | null } | 恢复的工作流事件数据
   */ async resume(params, options) {
        const apiUrl = '/v1/workflow/stream_resume';
        const response = await this._client.post(apiUrl, params, false, options);
        return response;
    }
    /**
   * Get the workflow run history | 工作流异步运行后，查看执行结果
   * @docs zh: https://www.coze.cn/open/docs/developer_guides/workflow_history
   * @param workflowId - Required The ID of the workflow. | 必选 工作流 ID。
   * @param executeId - Required The ID of the workflow execution. | 必选 工作流执行 ID。
   * @returns WorkflowExecuteHistory[] | 工作流执行历史
   */ async history(workflowId, executeId, options) {
        const apiUrl = `/v1/workflows/${workflowId}/run_histories/${executeId}`;
        const response = await this._client.get(apiUrl, void 0, false, options);
        return response.data;
    }
}
class WorkflowEvent {
    constructor(id, event, data){
        this.id = id;
        this.event = event;
        this.data = data;
    }
}
class WorkflowChat extends APIResource {
    /**
   * Execute a chat workflow. | 执行对话流
   * @docs en: https://www.coze.cn/docs/developer_guides/workflow_chat?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/workflow_chat?_lang=zh
   * @param params.workflow_id - Required The ID of the workflow to chat with. | 必选 要对话的工作流 ID。
   * @param params.additional_messages - Required Array of messages for the chat. | 必选 对话的消息数组。
   * @param params.parameters - Optional  Parameters for the workflow execution. | 必选 工作流执行的参数。
   * @param params.app_id - Optional The ID of the app. | 可选 应用 ID。
   * @param params.bot_id - Optional The ID of the bot. | 可选 Bot ID。
   * @param params.conversation_id - Optional The ID of the conversation. | 可选 会话 ID。
   * @param params.ext - Optional Additional information for the chat. | 可选 对话的附加信息。
   * @returns AsyncGenerator<StreamChatData> | 对话数据流
   */ async *stream(params, options) {
        const apiUrl = '/v1/workflows/chat';
        const payload = {
            ...params,
            additional_messages: handleAdditionalMessages(params.additional_messages)
        };
        const result = await this._client.post(apiUrl, payload, true, options);
        for await (const message of result)if (message.event === chat_ChatEventType.DONE) {
            const ret = {
                event: message.event,
                data: '[DONE]'
            };
            yield ret;
        } else try {
            const ret = {
                event: message.event,
                data: JSON.parse(message.data)
            };
            yield ret;
        } catch (error) {
            throw new CozeError(`Could not parse message into JSON:${message.data}`);
        }
    }
}
class Workflows extends APIResource {
    constructor(...args){
        super(...args), this.runs = new Runs(this._client), this.chat = new WorkflowChat(this._client);
    }
}
class WorkSpaces extends APIResource {
    /**
   * View the list of workspaces that the current Coze user has joined. | 查看当前扣子用户加入的空间列表。
   * @docs en: https://www.coze.com/docs/developer_guides/list_workspace?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/list_workspace?_lang=zh
   * @param params.page_num - Optional The page number for paginated queries. Default is 1.
   * | 可选 分页查询时的页码。默认为 1，即从第一页数据开始返回。
   * @param params.page_size - Optional The size of pagination. Default is 10. Maximum is 50. | 可选 分页大小。默认为 10，最大为 50。
   * @returns OpenSpaceData | 工作空间列表
   */ async list(params, options) {
        const apiUrl = '/v1/workspaces';
        const response = await this._client.get(apiUrl, params, false, options);
        return safeJsonParse(response, response).data;
    }
}
// Required header for knowledge APIs
const documents_headers = {
    'agw-js-conv': 'str'
};
class Documents extends APIResource {
    /**
   * @deprecated  The method is deprecated and will be removed in a future version. Please use 'client.datasets.documents.list' instead.
   *
   * View the file list of a specified knowledge base, which includes lists of documents, spreadsheets, or images.
   * | 调用接口查看指定知识库的内容列表，即文件、表格或图像列表。
   * @docs en: https://www.coze.com/docs/developer_guides/list_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/list_knowledge_files?_lang=zh
   * @param params.dataset_id - Required The ID of the knowledge base. | 必选 待查看文件的知识库 ID。
   * @param params.page - Optional The page number for paginated queries. Default is 1. | 可选 分页查询时的页码。默认为 1。
   * @param params.page_size - Optional The size of pagination. Default is 10. | 可选 分页大小。默认为 10。
   * @returns ListDocumentData | 知识库文件列表
   */ list(params, options) {
        const apiUrl = '/open_api/knowledge/document/list';
        const response = this._client.get(apiUrl, params, false, mergeConfig(options, {
            headers: documents_headers
        }));
        return response;
    }
    /**
   * @deprecated  The method is deprecated and will be removed in a future version. Please use 'client.datasets.documents.create' instead.
   *
   * Upload files to the specific knowledge. | 调用此接口向指定知识库中上传文件。
   * @docs en: https://www.coze.com/docs/developer_guides/create_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/create_knowledge_files?_lang=zh
   * @param params.dataset_id - Required The ID of the knowledge. | 必选 知识库 ID。
   * @param params.document_bases - Required The metadata information of the files awaiting upload. | 必选 待上传文件的元数据信息。
   * @param params.chunk_strategy - Required when uploading files to a new knowledge for the first time. Chunk strategy.
   * | 向新知识库首次上传文件时必选 分段规则。
   * @returns DocumentInfo[] | 已上传文件的基本信息
   */ async create(params, options) {
        const apiUrl = '/open_api/knowledge/document/create';
        const response = await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_headers
        }));
        return response.document_infos;
    }
    /**
   * @deprecated  The method is deprecated and will be removed in a future version. Please use 'client.datasets.documents.delete' instead.
   *
   * Delete text, images, sheets, and other files in the knowledge base, supporting batch deletion.
   * | 删除知识库中的文本、图像、表格等文件，支持批量删除。
   * @docs en: https://www.coze.com/docs/developer_guides/delete_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/delete_knowledge_files?_lang=zh
   * @param params.document_ids - Required The list of knowledge base files to be deleted. | 必选 待删除的文件 ID。
   * @returns void | 无返回
   */ async delete(params, options) {
        const apiUrl = '/open_api/knowledge/document/delete';
        await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_headers
        }));
    }
    /**
   * @deprecated  The method is deprecated and will be removed in a future version. Please use 'client.datasets.documents.update' instead.
   *
   * Modify the knowledge base file name and update strategy. | 调用接口修改知识库文件名称和更新策略。
   * @docs en: https://www.coze.com/docs/developer_guides/modify_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/modify_knowledge_files?_lang=zh
   * @param params.document_id - Required The ID of the knowledge base file. | 必选 待修改的知识库文件 ID。
   * @param params.document_name - Optional The new name of the knowledge base file. | 可选 知识库文件的新名称。
   * @param params.update_rule - Optional The update strategy for online web pages. | 可选 在线网页更新策略。
   * @returns void | 无返回
   */ async update(params, options) {
        const apiUrl = '/open_api/knowledge/document/update';
        await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_headers
        }));
    }
}
class Knowledge extends APIResource {
    constructor(...args){
        super(...args), /**
   * @deprecated
   */ this.documents = new Documents(this._client);
    }
}
// Required header for knowledge APIs
const documents_documents_headers = {
    'agw-js-conv': 'str'
};
class documents_Documents extends APIResource {
    /**
   * View the file list of a specified knowledge base, which includes lists of documents, spreadsheets, or images.
   * | 调用接口查看指定知识库的内容列表，即文件、表格或图像列表。
   * @docs en: https://www.coze.com/docs/developer_guides/list_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/list_knowledge_files?_lang=zh
   * @param params.dataset_id - Required The ID of the knowledge base. | 必选 待查看文件的知识库 ID。
   * @param params.page - Optional The page number for paginated queries. Default is 1. | 可选 分页查询时的页码。默认为 1。
   * @param params.page_size - Optional The size of pagination. Default is 10. | 可选 分页大小。默认为 10。
   * @returns ListDocumentData | 知识库文件列表
   */ async list(params, options) {
        const apiUrl = '/open_api/knowledge/document/list';
        const response = await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_documents_headers
        }));
        return response;
    }
    /**
   * Upload files to the specific knowledge. | 调用此接口向指定知识库中上传文件。
   * @docs en: https://www.coze.com/docs/developer_guides/create_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/create_knowledge_files?_lang=zh
   * @param params.dataset_id - Required The ID of the knowledge. | 必选 知识库 ID。
   * @param params.document_bases - Required The metadata information of the files awaiting upload. | 必选 待上传文件的元数据信息。
   * @param params.chunk_strategy - Required when uploading files to a new knowledge for the first time. Chunk strategy.
   * | 向新知识库首次上传文件时必选 分段规则。
   * @returns DocumentInfo[] | 已上传文件的基本信息
   */ async create(params, options) {
        const apiUrl = '/open_api/knowledge/document/create';
        const response = await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_documents_headers
        }));
        return response.document_infos;
    }
    /**
   * Delete text, images, sheets, and other files in the knowledge base, supporting batch deletion.
   * | 删除知识库中的文本、图像、表格等文件，支持批量删除。
   * @docs en: https://www.coze.com/docs/developer_guides/delete_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/delete_knowledge_files?_lang=zh
   * @param params.document_ids - Required The list of knowledge base files to be deleted. | 必选 待删除的文件 ID。
   * @returns void | 无返回
   */ async delete(params, options) {
        const apiUrl = '/open_api/knowledge/document/delete';
        await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_documents_headers
        }));
    }
    /**
   * Modify the knowledge base file name and update strategy. | 调用接口修改知识库文件名称和更新策略。
   * @docs en: https://www.coze.com/docs/developer_guides/modify_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/modify_knowledge_files?_lang=zh
   * @param params.document_id - Required The ID of the knowledge base file. | 必选 待修改的知识库文件 ID。
   * @param params.document_name - Optional The new name of the knowledge base file. | 可选 知识库文件的新名称。
   * @param params.update_rule - Optional The update strategy for online web pages. | 可选 在线网页更新策略。
   * @returns void | 无返回
   */ async update(params, options) {
        const apiUrl = '/open_api/knowledge/document/update';
        await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_documents_headers
        }));
    }
}
class Images extends APIResource {
    /**
   * Update the description of an image in the knowledge base | 更新知识库中的图片描述
   * @docs en: https://www.coze.com/docs/developer_guides/developer_guides/update_image_caption?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/developer_guides/update_image_caption?_lang=zh
   * @param datasetId - The ID of the dataset | 必选 知识库 ID
   * @param documentId - The ID of the document | 必选 知识库文件 ID
   * @param params - The parameters for updating the image
   * @param params.caption - Required. The description of the image | 必选 图片的描述信息
   * @returns undefined
   */ // eslint-disable-next-line max-params
    async update(datasetId, documentId, params, options) {
        const apiUrl = `/v1/datasets/${datasetId}/images/${documentId}`;
        await this._client.put(apiUrl, params, false, options);
    }
    /**
   * List images in the knowledge base | 列出知识库中的图片
   * @docs en: https://www.coze.com/docs/developer_guides/developer_guides/get_images?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/developer_guides/get_images?_lang=zh
   * @param datasetId - The ID of the dataset | 必选 知识库 ID
   * @param params - The parameters for listing images
   * @param params.page_num - Optional. Page number for pagination, minimum value is 1, defaults to 1 | 可选 分页查询时的页码。默认为 1。
   * @param params.page_size - Optional. Number of items per page, range 1-299, defaults to 10 | 可选 分页大小。默认为 10。
   * @param params.keyword - Optional. Search keyword for image descriptions | 可选 图片描述的搜索关键词。
   * @param params.has_caption - Optional. Filter for images with/without captions | 可选 是否过滤有/无描述的图片。
   */ async list(datasetId, params, options) {
        const apiUrl = `/v1/datasets/${datasetId}/images`;
        const response = await this._client.get(apiUrl, params, false, options);
        return response.data;
    }
}
class Datasets extends APIResource {
    /**
   * Creates a new dataset | 创建数据集
   * @docs en: https://www.coze.com/docs/developer_guides/create_dataset?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/create_dataset?_lang=zh
   * @param params - The parameters for creating a dataset
   * @param {string} params.name - Required. Dataset name, maximum length of 100 characters | 必选 数据集名称，最大长度为 100 个字符
   * @param {string} params.space_id - Required. Space ID where the dataset belongs | 必选 数据集所属的空间 ID
   * @param {number} params.format_type - Required. Dataset type (0: Text type, 2: Image type) | 必选 数据集类型 (0: 文本类型, 2: 图片类型)
   * @param {string} [params.description] - Optional. Dataset description | 可选 数据集描述
   * @param {string} [params.file_id] - Optional. Dataset icon file ID from file upload
   */ async create(params, options) {
        const apiUrl = '/v1/datasets';
        const response = await this._client.post(apiUrl, params, false, options);
        return response.data;
    }
    /**
   * Lists all datasets in a space | 列出空间中的所有数据集
   * @docs en: https://www.coze.com/docs/developer_guides/list_dataset?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/list_dataset?_lang=zh
   * @param params - The parameters for listing datasets | 列出数据集的参数
   * @param {string} params.space_id - Required. Space ID where the datasets belong | 必选 数据集所属的空间 ID
   * @param {string} [params.name] - Optional. Dataset name for fuzzy search | 可选 数据集名称用于模糊搜索
   * @param {number} [params.format_type] - Optional. Dataset type (0: Text type, 2: Image type) | 可选 数据集类型 (0: 文本类型, 2: 图片类型)
   * @param {number} [params.page_num] - Optional. Page number for pagination (default: 1) | 可选 分页查询时的页码。默认为 1。
   * @param {number} [params.page_size] - Optional. Number of items per page (default: 10) | 可选 分页大小。默认为 10。
   */ async list(params, options) {
        const apiUrl = '/v1/datasets';
        const response = await this._client.get(apiUrl, params, false, options);
        return response.data;
    }
    /**
   * Updates a dataset | 更新数据集
   * @docs en: https://www.coze.com/docs/developer_guides/update_dataset?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/update_dataset?_lang=zh
   * @param dataset_id - Required. The ID of the dataset to update | 必选 数据集 ID
   * @param params - Required. The parameters for updating the dataset | 必选 更新数据集的参数
   * @param params.name - Required. Dataset name, maximum length of 100 characters. | 必选 数据集名称，最大长度为 100 个字符。
   * @param params.file_id - Optional. Dataset icon, should pass the file_id obtained from the file upload interface. | 可选 数据集图标，应传递从文件上传接口获取的 file_id。
   * @param params.description - Optional. Dataset description. | 可选 数据集描述。
   */ async update(dataset_id, params, options) {
        const apiUrl = `/v1/datasets/${dataset_id}`;
        await this._client.put(apiUrl, params, false, options);
    }
    /**
   * Deletes a dataset | 删除数据集
   * @docs en: https://www.coze.com/docs/developer_guides/delete_dataset?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/delete_dataset?_lang=zh
   * @param dataset_id - Required. The ID of the dataset to delete | 必选 数据集 ID
   */ async delete(dataset_id, options) {
        const apiUrl = `/v1/datasets/${dataset_id}`;
        await this._client.delete(apiUrl, false, options);
    }
    /**
   * Views the progress of dataset upload | 查看数据集上传进度
   * @docs en: https://www.coze.com/docs/developer_guides/get_dataset_progress?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/get_dataset_progress?_lang=zh
   * @param dataset_id - Required. The ID of the dataset to process | 必选 数据集 ID
   * @param params - Required. The parameters for processing the dataset | 必选 处理数据集的参数
   * @param params.dataset_ids - Required. List of dataset IDs | 必选 数据集 ID 列表
   */ async process(dataset_id, params, options) {
        const apiUrl = `/v1/datasets/${dataset_id}/process`;
        const response = await this._client.post(apiUrl, params, false, options);
        return response.data;
    }
    constructor(...args){
        super(...args), this.documents = new documents_Documents(this._client), this.images = new Images(this._client);
    }
}
class Voices extends APIResource {
    /**
   * @description Clone a voice | 音色克隆
   * @param params
   * @param params.voice_name - Required. Voice name, cannot be empty and must be longer than 6 characters
   * | 复刻的音色名称，不能为空，长度大于 6
   * @param params.file - Required. Audio file | 音频文件
   * @param params.audio_format - Required. Only supports "wav", "mp3", "ogg", "m4a", "aac", "pcm" formats
   * | 只支持 "wav", "mp3", "ogg", "m4a", "aac", "pcm" 格式
   * @param params.language - Optional. Only supports "zh", "en" "ja" "es" "id" "pt" languages
   * | 只支持 "zh", "en" "ja" "es" "id" "pt" 语种
   * @param params.voice_id - Optional. If provided, will train on existing voice and override previous training
   * | 传入的话就会在原有的音色上去训练，覆盖前面训练好的音色
   * @param params.preview_text - Optional. If provided, will generate preview audio based on this text, otherwise uses default text
   * | 如果传入会基于该文本生成预览音频，否则使用默认的文本
   * @param params.text - Optional. Users can read this text, service will compare audio with text. Returns error if difference is too large
   * | 可以让用户按照该文本念诵，服务会对比音频与该文本的差异。若差异过大会返回错误
   * @param params.space_id - Optional.  The space id of the voice. | 空间ID
   * @param params.description- Optional. The description of the voice. | 音色描述
   * @param options - Request options
   * @returns Clone voice data
   */ async clone(params, options) {
        const apiUrl = '/v1/audio/voices/clone';
        const response = await this._client.post(apiUrl, (0, external_axios_namespaceObject.toFormData)(params), false, options);
        return response.data;
    }
    /**
   * @description List voices | 获取音色列表
   * @param params
   * @param params.filter_system_voice - Optional. Whether to filter system voices, default is false
   * | 是否过滤系统音色, 默认不过滤
   * @param params.page_num - Optional. Starts from 1 by default, value must be > 0
   * | 不传默认从 1 开始，传值需要 > 0
   * @param params.page_size - Optional. Default is 100, value must be (0, 100]
   * | 不传默认 100，传值需要 (0, 100]
   * @param options - Request options
   * @returns List voices data
   */ async list(params, options) {
        const apiUrl = '/v1/audio/voices';
        const response = await this._client.get(apiUrl, params, false, options);
        return response.data;
    }
}
class Transcriptions extends APIResource {
    /**
   * ASR voice to text | ASR 语音转文本
   * @param params - Required The parameters for file upload | 上传文件所需的参数
   * @param params.file - Required The audio file to be uploaded. | 需要上传的音频文件。
   */ async create(params, options) {
        const apiUrl = '/v1/audio/transcriptions';
        const response = await this._client.post(apiUrl, (0, external_axios_namespaceObject.toFormData)(params), false, options);
        return response.data;
    }
}
class Speech extends APIResource {
    /**
   * @description Speech synthesis | 语音合成
   * @param params
   * @param params.input - Required. Text to generate audio | 要为其生成音频的文本
   * @param params.voice_id - Required. Voice ID | 生成音频的音色 ID
   * @param params.response_format - Optional. Audio encoding format,
   * supports "wav", "pcm", "ogg", "opus", "mp3", default is "mp3"
   * | 音频编码格式，支持 "wav", "pcm", "ogg", "opus", "mp3"，默认是 "mp3"
   * @param options - Request options
   * @returns Speech synthesis data
   */ async create(params, options) {
        const apiUrl = '/v1/audio/speech';
        const response = await this._client.post(apiUrl, {
            ...params,
            sample_rate: params.sample_rate || 24000
        }, false, mergeConfig(options, {
            responseType: 'arraybuffer'
        }));
        return response;
    }
}
class Rooms extends APIResource {
    async create(params, options) {
        const apiUrl = '/v1/audio/rooms';
        const response = await this._client.post(apiUrl, params, false, options);
        return response.data;
    }
}
class Audio extends APIResource {
    constructor(...args){
        super(...args), this.rooms = new Rooms(this._client), this.voices = new Voices(this._client), this.speech = new Speech(this._client), this.transcriptions = new Transcriptions(this._client);
    }
}
class Templates extends APIResource {
    /**
   * Duplicate a template. | 复制一个模板。
   * @param templateId - Required. The ID of the template to duplicate. | 要复制的模板的 ID。
   * @param params - Optional. The parameters for the duplicate operation. | 可选参数，用于复制操作。
   * @param params.workspace_id - Required. The ID of the workspace to duplicate the template into. | 要复制到的目标工作空间的 ID。
   * @param params.name - Optional. The name of the new template. | 新模板的名称。
   * @returns TemplateDuplicateRes | 复制模板结果
   */ async duplicate(templateId, params, options) {
        const apiUrl = `/v1/templates/${templateId}/duplicate`;
        const response = await this._client.post(apiUrl, params, false, options);
        return response.data;
    }
}
class chat_Chat extends APIResource {
    async create(req, options) {
        const apiUrl = buildWebsocketUrl('/v1/chat', req);
        return await this._client.makeWebsocket(apiUrl, options);
    }
}
class transcriptions_Transcriptions extends APIResource {
    async create(options) {
        const apiUrl = '/v1/audio/transcriptions';
        return await this._client.makeWebsocket(apiUrl, options);
    }
}
class speech_Speech extends APIResource {
    async create(options) {
        const apiUrl = '/v1/audio/speech';
        return await this._client.makeWebsocket(apiUrl, options);
    }
}
class audio_Audio extends APIResource {
    constructor(...args){
        super(...args), this.speech = new speech_Speech(this._client), this.transcriptions = new transcriptions_Transcriptions(this._client);
    }
}
// Common types (not exported)
// Keep all existing exports but use the base types where applicable
var types_WebsocketsEventType = /*#__PURE__*/ function(WebsocketsEventType) {
    // Common
    /** SDK error */ WebsocketsEventType["CLIENT_ERROR"] = "client_error";
    /** Connection closed */ WebsocketsEventType["CLOSED"] = "closed";
    /** All events */ WebsocketsEventType["ALL"] = "all";
    // Error
    /** Received error event */ WebsocketsEventType["ERROR"] = "error";
    // v1/audio/speech
    /** Send text to server */ WebsocketsEventType["INPUT_TEXT_BUFFER_APPEND"] = "input_text_buffer.append";
    /** No text to send, after audio all received, can close connection */ WebsocketsEventType["INPUT_TEXT_BUFFER_COMPLETE"] = "input_text_buffer.complete";
    /** Send speech config to server */ WebsocketsEventType["SPEECH_UPDATE"] = "speech.update";
    /** Received `speech.updated` event */ WebsocketsEventType["SPEECH_UPDATED"] = "speech.updated";
    /** After speech created */ WebsocketsEventType["SPEECH_CREATED"] = "speech.created";
    /** Received `input_text_buffer.complete` event */ WebsocketsEventType["INPUT_TEXT_BUFFER_COMPLETED"] = "input_text_buffer.completed";
    /** Received `speech.update` event */ WebsocketsEventType["SPEECH_AUDIO_UPDATE"] = "speech.audio.update";
    /** All audio received, can close connection */ WebsocketsEventType["SPEECH_AUDIO_COMPLETED"] = "speech.audio.completed";
    // v1/audio/transcriptions
    /** Send audio to server */ WebsocketsEventType["INPUT_AUDIO_BUFFER_APPEND"] = "input_audio_buffer.append";
    /** No audio to send, after text all received, can close connection */ WebsocketsEventType["INPUT_AUDIO_BUFFER_COMPLETE"] = "input_audio_buffer.complete";
    /** Send transcriptions config to server */ WebsocketsEventType["TRANSCRIPTIONS_UPDATE"] = "transcriptions.update";
    /** Send `input_audio_buffer.clear` event */ WebsocketsEventType["INPUT_AUDIO_BUFFER_CLEAR"] = "input_audio_buffer.clear";
    /** After transcriptions created */ WebsocketsEventType["TRANSCRIPTIONS_CREATED"] = "transcriptions.created";
    /** Received `input_audio_buffer.complete` event */ WebsocketsEventType["INPUT_AUDIO_BUFFER_COMPLETED"] = "input_audio_buffer.completed";
    /** Received `transcriptions.update` event */ WebsocketsEventType["TRANSCRIPTIONS_MESSAGE_UPDATE"] = "transcriptions.message.update";
    /** All audio received, can close connection */ WebsocketsEventType["TRANSCRIPTIONS_MESSAGE_COMPLETED"] = "transcriptions.message.completed";
    /** Received `input_audio_buffer.cleared` event */ WebsocketsEventType["INPUT_AUDIO_BUFFER_CLEARED"] = "input_audio_buffer.cleared";
    /** Received `transcriptions.updated` event */ WebsocketsEventType["TRANSCRIPTIONS_UPDATED"] = "transcriptions.updated";
    // v1/chat
    /** Send chat config to server */ WebsocketsEventType["CHAT_UPDATE"] = "chat.update";
    /** Send tool outputs to server */ WebsocketsEventType["CONVERSATION_CHAT_SUBMIT_TOOL_OUTPUTS"] = "conversation.chat.submit_tool_outputs";
    /** After chat created */ WebsocketsEventType["CHAT_CREATED"] = "chat.created";
    /** After chat updated */ WebsocketsEventType["CHAT_UPDATED"] = "chat.updated";
    /** Audio AST completed, chat started */ WebsocketsEventType["CONVERSATION_CHAT_CREATED"] = "conversation.chat.created";
    /** Message created */ WebsocketsEventType["CONVERSATION_MESSAGE_CREATE"] = "conversation.message.create";
    /** Clear conversation */ WebsocketsEventType["CONVERSATION_CLEAR"] = "conversation.clear";
    /** Chat in progress */ WebsocketsEventType["CONVERSATION_CHAT_IN_PROGRESS"] = "conversation.chat.in_progress";
    /** Get agent text message update */ WebsocketsEventType["CONVERSATION_MESSAGE_DELTA"] = "conversation.message.delta";
    /** Need plugin submit */ WebsocketsEventType["CONVERSATION_CHAT_REQUIRES_ACTION"] = "conversation.chat.requires_action";
    /** Message completed */ WebsocketsEventType["CONVERSATION_MESSAGE_COMPLETED"] = "conversation.message.completed";
    /** Get agent audio message update */ WebsocketsEventType["CONVERSATION_AUDIO_DELTA"] = "conversation.audio.delta";
    /** Audio message completed */ WebsocketsEventType["CONVERSATION_AUDIO_COMPLETED"] = "conversation.audio.completed";
    /** All message received, can close connection */ WebsocketsEventType["CONVERSATION_CHAT_COMPLETED"] = "conversation.chat.completed";
    /** Chat failed */ WebsocketsEventType["CONVERSATION_CHAT_FAILED"] = "conversation.chat.failed";
    /** Received `conversation.cleared` event */ WebsocketsEventType["CONVERSATION_CLEARED"] = "conversation.cleared";
    /** Speech started */ WebsocketsEventType["INPUT_AUDIO_BUFFER_SPEECH_STARTED"] = "input_audio_buffer.speech_started";
    /** Speech stopped */ WebsocketsEventType["INPUT_AUDIO_BUFFER_SPEECH_STOPPED"] = "input_audio_buffer.speech_stopped";
    /** Chat interrupted by client */ WebsocketsEventType["CONVERSATION_CHAT_CANCEL"] = "conversation.chat.cancel";
    /** Chat canceled */ WebsocketsEventType["CONVERSATION_CHAT_CANCELED"] = "conversation.chat.canceled";
    /** Audio transcript update */ WebsocketsEventType["CONVERSATION_AUDIO_TRANSCRIPT_UPDATE"] = "conversation.audio_transcript.update";
    /** Audio transcript completed */ WebsocketsEventType["CONVERSATION_AUDIO_TRANSCRIPT_COMPLETED"] = "conversation.audio_transcript.completed";
    /** Audio dump */ WebsocketsEventType["DUMP_AUDIO"] = "dump.audio";
    return WebsocketsEventType;
}({});
class Websockets extends APIResource {
    constructor(...args){
        super(...args), this.audio = new audio_Audio(this._client), this.chat = new chat_Chat(this._client);
    }
}
const external_ws_namespaceObject = require("ws");
var external_ws_default = /*#__PURE__*/ __webpack_require__.n(external_ws_namespaceObject);
const external_reconnecting_websocket_namespaceObject = require("reconnecting-websocket");
var external_reconnecting_websocket_default = /*#__PURE__*/ __webpack_require__.n(external_reconnecting_websocket_namespaceObject);
class WebSocketAPI {
    // Standard WebSocket properties
    get readyState() {
        return this.rws.readyState;
    }
    // Standard WebSocket methods
    send(data) {
        return this.rws.send(JSON.stringify(data));
    }
    close(code, reason) {
        return this.rws.close(code, reason);
    }
    reconnect(code, reason) {
        return this.rws.reconnect(code, reason);
    }
    // Event listener methods
    addEventListener(type, listener) {
        this.rws.addEventListener(type, listener);
    }
    removeEventListener(type, listener) {
        this.rws.removeEventListener(type, listener);
    }
    constructor(url, options = {}){
        // Event handler methods
        this.onmessage = null;
        this.onopen = null;
        this.onclose = null;
        this.onerror = null;
        const separator = url.includes('?') ? '&' : '?';
        const { authorization } = options.headers || {};
        this.rws = new (external_reconnecting_websocket_default())(`${url}${separator}authorization=${authorization}`, [], {
            WebSocket: utils_isBrowser() ? window.WebSocket : class extends external_ws_default() {
                constructor(url2, protocols){
                    super(url2, protocols, {
                        headers: options.headers
                    });
                }
            },
            ...options
        });
        this.rws.addEventListener('message', (event)=>{
            try {
                var _this_onmessage, _this;
                const data = JSON.parse(event.data);
                null === (_this_onmessage = (_this = this).onmessage) || void 0 === _this_onmessage || _this_onmessage.call(_this, data, event);
            } catch (error) {
                console.error('WebSocketAPI onmessage error', error);
            }
        });
        this.rws.addEventListener('open', (event)=>{
            var _this_onopen, _this;
            null === (_this_onopen = (_this = this).onopen) || void 0 === _this_onopen || _this_onopen.call(_this, event);
        });
        this.rws.addEventListener('close', (event)=>{
            var _this_onclose, _this;
            null === (_this_onclose = (_this = this).onclose) || void 0 === _this_onclose || _this_onclose.call(_this, event);
        });
        this.rws.addEventListener('error', (event)=>{
            var _event_target__req_res, _event_target__req, _event_target, _event_target__req_res1, _event_target__req1, _event_target1, _this_onerror, _this;
            const { readyState } = this.rws;
            if (3 === readyState) return;
            const statusCode = null === (_event_target = event.target) || void 0 === _event_target ? void 0 : null === (_event_target__req = _event_target._req) || void 0 === _event_target__req ? void 0 : null === (_event_target__req_res = _event_target__req.res) || void 0 === _event_target__req_res ? void 0 : _event_target__req_res.statusCode;
            const rawHeaders = (null === (_event_target1 = event.target) || void 0 === _event_target1 ? void 0 : null === (_event_target__req1 = _event_target1._req) || void 0 === _event_target__req1 ? void 0 : null === (_event_target__req_res1 = _event_target__req1.res) || void 0 === _event_target__req_res1 ? void 0 : _event_target__req_res1.rawHeaders) || [];
            const logidIndex = rawHeaders.findIndex((header)=>'X-Tt-Logid' === header);
            const logid = -1 !== logidIndex ? rawHeaders[logidIndex + 1] : void 0;
            const error = {
                id: '0',
                event_type: types_WebsocketsEventType.ERROR,
                data: {
                    code: -1,
                    msg: 'WebSocket error'
                },
                detail: {
                    logid
                }
            };
            if (401 === statusCode) {
                error.data.code = 401;
                error.data.msg = 'Unauthorized';
            } else if (403 === statusCode) {
                error.data.code = 403;
                error.data.msg = 'Forbidden';
            } else {
                error.data.code = 500;
                var _event_error;
                error.data.msg = String(null !== (_event_error = null == event ? void 0 : event.error) && void 0 !== _event_error ? _event_error : '') || 'WebSocket error';
            }
            null === (_this_onerror = (_this = this).onerror) || void 0 === _this_onerror || _this_onerror.call(_this, error, event);
        });
    }
}
const external_os_namespaceObject = require("os");
var external_os_default = /*#__PURE__*/ __webpack_require__.n(external_os_namespaceObject);
var package_namespaceObject = JSON.parse('{"name":"@coze/api","version":"1.2.0","description":"Official Coze Node.js SDK for seamless AI integration into your applications | 扣子官方 Node.js SDK，助您轻松集成 AI 能力到应用中","keywords":["coze","ai","nodejs","sdk","chatbot","typescript"],"homepage":"https://github.com/coze-dev/coze-js/tree/main/packages/coze-js","bugs":{"url":"https://github.com/coze-dev/coze-js/issues"},"repository":{"type":"git","url":"https://github.com/coze-dev/coze-js.git","directory":"packages/coze-js"},"license":"MIT","author":"Leeight <leeight@gmail.com>","exports":{".":{"require":"./dist/cjs/index.js","import":"./dist/esm/index.mjs","types":"./dist/types/index.d.ts"},"./ws-tools":{"require":"./dist/cjs/ws-tools/index.js","import":"./dist/esm/ws-tools/index.mjs","types":"./dist/types/ws-tools/ws-tools/index.d.ts"}},"main":"dist/cjs/index.js","unpkg":"dist/umd/index.js","module":"dist/esm/index.mjs","browser":{"crypto":false,"os":false,"jsonwebtoken":false,"node-fetch":false,"ws":false},"typesVersions":{"*":{".":["dist/types/index.d.ts"],"ws-tools":["dist/types/ws-tools/ws-tools/index.d.ts"]}},"files":["dist","LICENSE","README.md","README.zh-CN.md"],"scripts":{"build":"rslib build","format":"prettier --write .","lint":"eslint ./ --cache --quiet","start":"rslib build -w","test":"vitest","test:cov":"vitest --coverage --run"},"dependencies":{"agora-extension-ai-denoiser":"^1.0.0","agora-rtc-sdk-ng":"^4.23.2","agora-rte-extension":"^1.2.4","jsonwebtoken":"^9.0.2","node-fetch":"^2.x","reconnecting-websocket":"^4.4.0","uuid":"^10.0.0","ws":"^8.11.0"},"devDependencies":{"@coze-infra/eslint-config":"workspace:*","@coze-infra/ts-config":"workspace:*","@coze-infra/vitest-config":"workspace:*","@rslib/core":"0.0.18","@swc/core":"^1.3.14","@types/jsonwebtoken":"^9.0.0","@types/node":"^20","@types/node-fetch":"^2.x","@types/uuid":"^9.0.1","@types/whatwg-fetch":"^0.0.33","@types/ws":"^8.5.1","@vitest/coverage-v8":"~2.1.4","axios":"^1.7.7","typescript":"^5.5.3","vitest":"~2.1.4"},"peerDependencies":{"axios":"^1.7.1"},"cozePublishConfig":{"exports":{".":{"require":"./dist/cjs/index.js","import":"./dist/esm/index.mjs","types":"./dist/types/index.d.ts"},"./ws-tools":{"require":"./dist/cjs/ws-tools/index.js","import":"./dist/esm/ws-tools/index.mjs","types":"./dist/types/ws-tools/ws-tools/index.d.ts"}},"main":"dist/cjs/index.js","module":"dist/esm/index.mjs","types":"dist/types/index.d.ts"},"types":"dist/types/index.d.ts"}'); // CONCATENATED MODULE: ./src/version.ts
const { version: version_version } = package_namespaceObject;
const getEnv = ()=>{
    const nodeVersion = process.version.slice(1); // Remove 'v' prefix
    const { platform } = process;
    let osName = platform.toLowerCase();
    let osVersion = external_os_default().release();
    if ('darwin' === platform) {
        osName = 'macos';
        // Try to parse the macOS version
        try {
            const darwinVersion = external_os_default().release().split('.');
            if (darwinVersion.length >= 2) {
                const majorVersion = parseInt(darwinVersion[0], 10);
                if (!isNaN(majorVersion) && majorVersion >= 9) {
                    const macVersion = majorVersion - 9;
                    osVersion = `10.${macVersion}.${darwinVersion[1]}`;
                }
            }
        } catch (error) {
        // Keep the default os.release() value if parsing fails
        }
    } else if ('win32' === platform) {
        osName = 'windows';
        osVersion = external_os_default().release();
    } else if ('linux' === platform) {
        osName = 'linux';
        osVersion = external_os_default().release();
    }
    return {
        osName,
        osVersion,
        nodeVersion
    };
};
const getUserAgent = ()=>{
    const { nodeVersion, osName, osVersion } = getEnv();
    return `coze-js/${version_version} node/${nodeVersion} ${osName}/${osVersion}`.toLowerCase();
};
const getNodeClientUserAgent = ()=>{
    const { osVersion, nodeVersion, osName } = getEnv();
    const ua = {
        version: version_version,
        lang: 'node',
        lang_version: nodeVersion,
        os_name: osName,
        os_version: osVersion
    };
    return JSON.stringify(ua);
};
const getBrowserClientUserAgent = ()=>{
    const browserInfo = {
        name: 'unknown',
        version: 'unknown'
    };
    const osInfo = {
        name: 'unknown',
        version: 'unknown'
    };
    const { userAgent } = navigator;
    // 检测操作系统及版本
    if (userAgent.indexOf('Windows') > -1) {
        var _userAgent_match;
        osInfo.name = 'windows';
        const windowsVersion = (null === (_userAgent_match = userAgent.match(/Windows NT ([0-9.]+)/)) || void 0 === _userAgent_match ? void 0 : _userAgent_match[1]) || 'unknown';
        osInfo.version = windowsVersion;
    } else if (userAgent.indexOf('Mac OS X') > -1) {
        var _userAgent_match1;
        osInfo.name = 'macos';
        // 将 10_15_7 格式转换为 10.15.7
        osInfo.version = ((null === (_userAgent_match1 = userAgent.match(/Mac OS X ([0-9_]+)/)) || void 0 === _userAgent_match1 ? void 0 : _userAgent_match1[1]) || 'unknown').replace(/_/g, '.');
    } else if (userAgent.indexOf('Linux') > -1) {
        var _userAgent_match2;
        osInfo.name = 'linux';
        osInfo.version = (null === (_userAgent_match2 = userAgent.match(/Linux ([0-9.]+)/)) || void 0 === _userAgent_match2 ? void 0 : _userAgent_match2[1]) || 'unknown';
    }
    // 检测浏览器及版本
    if (userAgent.indexOf('Chrome') > -1) {
        var _userAgent_match3;
        browserInfo.name = 'chrome';
        browserInfo.version = (null === (_userAgent_match3 = userAgent.match(/Chrome\/([0-9.]+)/)) || void 0 === _userAgent_match3 ? void 0 : _userAgent_match3[1]) || 'unknown';
    } else if (userAgent.indexOf('Firefox') > -1) {
        var _userAgent_match4;
        browserInfo.name = 'firefox';
        browserInfo.version = (null === (_userAgent_match4 = userAgent.match(/Firefox\/([0-9.]+)/)) || void 0 === _userAgent_match4 ? void 0 : _userAgent_match4[1]) || 'unknown';
    } else if (userAgent.indexOf('Safari') > -1) {
        var _userAgent_match5;
        browserInfo.name = 'safari';
        browserInfo.version = (null === (_userAgent_match5 = userAgent.match(/Version\/([0-9.]+)/)) || void 0 === _userAgent_match5 ? void 0 : _userAgent_match5[1]) || 'unknown';
    }
    const ua = {
        version: version_version,
        browser: browserInfo.name,
        browser_version: browserInfo.version,
        os_name: osInfo.name,
        os_version: osInfo.version
    };
    return JSON.stringify(ua);
};
// Get UniApp client user agent
const getUniAppClientUserAgent = ()=>{
    // Get system info
    const systemInfo = uni.getSystemInfoSync();
    const platformInfo = {
        name: 'unknown',
        version: 'unknown'
    };
    const osInfo = {
        name: 'unknown',
        version: 'unknown'
    };
    // Handle operating system info
    if ('android' === systemInfo.platform) {
        osInfo.name = 'android';
        osInfo.version = systemInfo.system || 'unknown';
    } else if ('ios' === systemInfo.platform) {
        osInfo.name = 'ios';
        osInfo.version = systemInfo.system || 'unknown';
    } else if ('windows' === systemInfo.platform) {
        osInfo.name = 'windows';
        osInfo.version = systemInfo.system || 'unknown';
    } else if ('mac' === systemInfo.platform) {
        osInfo.name = 'macos';
        osInfo.version = systemInfo.system || 'unknown';
    } else {
        // Other platforms use platform name directly
        osInfo.name = systemInfo.platform;
        osInfo.version = systemInfo.system || 'unknown';
    }
    // Handle app/platform info
    if (systemInfo.AppPlatform) {
        // App environment
        platformInfo.name = systemInfo.AppPlatform.toLowerCase();
        platformInfo.version = systemInfo.appVersion || 'unknown';
    } else if (systemInfo.uniPlatform) {
        // UniApp recognized platform
        platformInfo.name = systemInfo.uniPlatform;
        platformInfo.version = systemInfo.SDKVersion || 'unknown';
    } else {
        // Try to determine platform type from environment
        const { appName, appVersion } = systemInfo;
        if (appName) {
            platformInfo.name = appName.toLowerCase();
            platformInfo.version = appVersion || 'unknown';
        }
    }
    const ua = {
        version: version_version,
        framework: 'uniapp',
        platform: platformInfo.name,
        platform_version: platformInfo.version,
        os_name: osInfo.name,
        os_version: osInfo.version,
        screen_width: systemInfo.screenWidth,
        screen_height: systemInfo.screenHeight,
        device_model: systemInfo.model,
        device_brand: systemInfo.brand
    };
    return JSON.stringify(ua);
};
const external_node_fetch_namespaceObject = require("node-fetch");
var external_node_fetch_default = /*#__PURE__*/ __webpack_require__.n(external_node_fetch_namespaceObject);
/* eslint-disable @typescript-eslint/no-explicit-any */ const handleError = (error)=>{
    if (!error.isAxiosError && (!error.code || !error.message)) return new CozeError(`Unexpected error: ${error.message}`);
    if ('ECONNABORTED' === error.code && error.message.includes('timeout') || 'ETIMEDOUT' === error.code) {
        var _error_response;
        return new TimeoutError(408, void 0, `Request timed out: ${error.message}`, null === (_error_response = error.response) || void 0 === _error_response ? void 0 : _error_response.headers);
    }
    if ('ERR_CANCELED' === error.code) return new APIUserAbortError(error.message);
    else {
        var _error_response1, _error_response2, _error_response3;
        return error_APIError.generate((null === (_error_response1 = error.response) || void 0 === _error_response1 ? void 0 : _error_response1.status) || 500, null === (_error_response2 = error.response) || void 0 === _error_response2 ? void 0 : _error_response2.data, error.message, null === (_error_response3 = error.response) || void 0 === _error_response3 ? void 0 : _error_response3.headers);
    }
};
// node-fetch is used for streaming requests
const adapterFetch = async (options)=>{
    const response = await external_node_fetch_default()(options.url, {
        body: options.data,
        ...options
    });
    return {
        data: response.body,
        ...response
    };
};
const isSupportNativeFetch = ()=>{
    if (utils_isBrowser() || utils_isUniApp()) return true;
    // native fetch is supported in node 18.0.0 or higher
    const version = process.version.slice(1);
    return compareVersions(version, '18.0.0') >= 0;
};
async function fetchAPI(url) {
    let options = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : {};
    const axiosInstance = options.axiosInstance || external_axios_default();
    // Add version check for streaming requests
    if (options.isStreaming && isAxiosStatic(axiosInstance)) {
        const axiosVersion = axiosInstance.VERSION || external_axios_default().VERSION;
        if (!axiosVersion || compareVersions(axiosVersion, '1.7.1') < 0) throw new CozeError('Streaming requests require axios version 1.7.1 or higher. Please upgrade your axios version.');
    }
    const response = await axiosInstance({
        url,
        responseType: options.isStreaming ? 'stream' : 'json',
        adapter: options.isStreaming ? isSupportNativeFetch() ? 'fetch' : adapterFetch : void 0,
        ...options
    }).catch((error)=>{
        throw handleError(error);
    });
    return {
        async *stream () {
            try {
                const stream = response.data;
                const reader = stream[Symbol.asyncIterator] ? stream[Symbol.asyncIterator]() : stream.getReader();
                const decoder = new TextDecoder();
                const fieldValues = {};
                let buffer = '';
                while(true){
                    const { done, value } = await (reader.next ? reader.next() : reader.read());
                    if (done) {
                        if (buffer) {
                            // If the stream ends without a newline, it means an error occurred
                            fieldValues.event = 'error';
                            fieldValues.data = buffer;
                            yield fieldValues;
                        }
                        break;
                    }
                    buffer += decoder.decode(value, {
                        stream: true
                    });
                    const lines = buffer.split('\n');
                    for(let i = 0; i < lines.length - 1; i++){
                        const line = lines[i];
                        const index = line.indexOf(':');
                        if (-1 !== index) {
                            const field = line.substring(0, index).trim();
                            const content = line.substring(index + 1).trim();
                            fieldValues[field] = content;
                            if ('data' === field) yield fieldValues;
                        }
                    }
                    buffer = lines[lines.length - 1]; // Keep the last incomplete line in the buffer
                }
            } catch (error) {
                handleError(error);
            }
        },
        json: ()=>response.data,
        response
    };
}
// Add version comparison utility
function compareVersions(v1, v2) {
    const v1Parts = v1.split('.').map(Number);
    const v2Parts = v2.split('.').map(Number);
    for(let i = 0; i < 3; i++){
        const part1 = v1Parts[i] || 0;
        const part2 = v2Parts[i] || 0;
        if (part1 > part2) return 1;
        if (part1 < part2) return -1;
    }
    return 0;
}
function isAxiosStatic(instance) {
    return !!(null == instance ? void 0 : instance.Axios);
}
/**
 * default coze  base URL is api.coze.com
 */ const constant_COZE_COM_BASE_URL = 'https://api.coze.com';
/**
 * change to wss://ws.coze.cn if you use https://coze.cn
 */ const COZE_CN_BASE_WS_URL = 'wss://ws.coze.cn';
/* eslint-disable max-params */ class core_APIClient {
    async getToken() {
        if ('function' == typeof this.token) return await this.token();
        return this.token;
    }
    async buildOptions(method, body, options) {
        const token = await this.getToken();
        const headers = {
            authorization: `Bearer ${token}`
        };
        if (utils_isUniApp()) headers['X-Coze-Client-User-Agent'] = getUniAppClientUserAgent();
        else if (utils_isBrowser()) headers['X-Coze-Client-User-Agent'] = getBrowserClientUserAgent();
        else {
            headers['User-Agent'] = getUserAgent();
            headers['X-Coze-Client-User-Agent'] = getNodeClientUserAgent();
        }
        const config = mergeConfig(this.axiosOptions, options, {
            headers
        }, {
            headers: this.headers || {}
        });
        config.method = method;
        config.data = body;
        return config;
    }
    async buildWebsocketOptions(options) {
        const token = await this.getToken();
        const headers = {
            authorization: `Bearer ${token}`
        };
        if (utils_isBrowser()) headers['X-Coze-Client-User-Agent'] = getBrowserClientUserAgent();
        else {
            headers['User-Agent'] = getUserAgent();
            headers['X-Coze-Client-User-Agent'] = getNodeClientUserAgent();
        }
        var _this__config_debug;
        const config = mergeConfig({
            debug: null !== (_this__config_debug = this._config.debug) && void 0 !== _this__config_debug && _this__config_debug
        }, this._config.websocketOptions, options, {
            headers
        }, {
            headers: this.headers || {}
        });
        return config;
    }
    async makeRequest(apiUrl, method, body, isStream, options) {
        const fullUrl = `${this.baseURL}${apiUrl}`;
        const fetchOptions = await this.buildOptions(method, body, options);
        fetchOptions.isStreaming = isStream;
        fetchOptions.axiosInstance = this.axiosInstance;
        this.debugLog(null == options ? void 0 : options.debug, `--- request url: ${fullUrl}`);
        this.debugLog(null == options ? void 0 : options.debug, '--- request options:', fetchOptions);
        const { response, stream, json } = await fetchAPI(fullUrl, fetchOptions);
        this.debugLog(null == options ? void 0 : options.debug, `--- response status: ${response.status}`);
        this.debugLog(null == options ? void 0 : options.debug, '--- response headers: ', response.headers);
        var _response_headers;
        // Taro use `header`
        const contentType = (null !== (_response_headers = response.headers) && void 0 !== _response_headers ? _response_headers : response.header)['content-type'];
        if (isStream) {
            if (contentType && contentType.includes('application/json')) {
                const result = await json();
                const { code, msg } = result;
                if (0 !== code && void 0 !== code) throw error_APIError.generate(response.status, result, msg, response.headers);
            }
            return stream();
        }
        if (!(contentType && contentType.includes('application/json'))) return await response.data;
        {
            const result = await json();
            const { code, msg } = result;
            if (0 !== code && void 0 !== code) throw error_APIError.generate(response.status, result, msg, response.headers);
            return result;
        }
    }
    async post(apiUrl, body) {
        let isStream = arguments.length > 2 && void 0 !== arguments[2] && arguments[2], options = arguments.length > 3 ? arguments[3] : void 0;
        return this.makeRequest(apiUrl, 'POST', body, isStream, options);
    }
    async get(apiUrl, param, isStream, options) {
        // 拼接参数
        const queryString = Object.entries(param || {}).map((param)=>{
            let [key, value] = param;
            return `${key}=${value}`;
        }).join('&');
        return this.makeRequest(queryString ? `${apiUrl}${apiUrl.includes('?') ? '&' : '?'}${queryString}` : apiUrl, 'GET', void 0, isStream, options);
    }
    async put(apiUrl, body, isStream, options) {
        return this.makeRequest(apiUrl, 'PUT', body, isStream, options);
    }
    async delete(apiUrl, isStream, options) {
        return this.makeRequest(apiUrl, 'DELETE', void 0, isStream, options);
    }
    async makeWebsocket(apiUrl, options) {
        const fullUrl = `${this.baseWsURL}${apiUrl}`;
        const websocketOptions = await this.buildWebsocketOptions(options);
        this.debugLog(null == options ? void 0 : options.debug, `--- websocket url: ${fullUrl}`);
        this.debugLog(null == options ? void 0 : options.debug, '--- websocket options:', websocketOptions);
        const ws = new WebSocketAPI(fullUrl, websocketOptions);
        return ws;
    }
    getConfig() {
        return this._config;
    }
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    debugLog() {
        let forceDebug = arguments.length > 0 && void 0 !== arguments[0] && arguments[0];
        for(var _len = arguments.length, msgs = new Array(_len > 1 ? _len - 1 : 0), _key = 1; _key < _len; _key++)msgs[_key - 1] = arguments[_key];
        if (this.debug || forceDebug) console.debug(...msgs);
    }
    constructor(config){
        this._config = config;
        this.baseURL = config.baseURL || constant_COZE_COM_BASE_URL;
        this.baseWsURL = config.baseWsURL || COZE_CN_BASE_WS_URL;
        this.token = config.token;
        this.axiosOptions = config.axiosOptions || {};
        this.axiosInstance = config.axiosInstance;
        this.debug = config.debug || false;
        this.allowPersonalAccessTokenInBrowser = config.allowPersonalAccessTokenInBrowser || false;
        this.headers = config.headers;
        if (utils_isBrowser() && 'function' != typeof this.token && isPersonalAccessToken(this.token) && !this.allowPersonalAccessTokenInBrowser) throw new CozeError('Browser environments do not support authentication using Personal Access Token (PAT) by default.\nas it may expose secret API keys. \n\nPlease use OAuth2.0 authentication mechanism. see:\nhttps://www.coze.com/docs/developer_guides/oauth_apps?_lang=en \n\nIf you need to force use, please set the `allowPersonalAccessTokenInBrowser` option to `true`. \n\ne.g new CozeAPI({ token, allowPersonalAccessTokenInBrowser: true });\n\n');
    }
}
core_APIClient.APIError = error_APIError;
core_APIClient.BadRequestError = BadRequestError;
core_APIClient.AuthenticationError = AuthenticationError;
core_APIClient.PermissionDeniedError = PermissionDeniedError;
core_APIClient.NotFoundError = NotFoundError;
core_APIClient.RateLimitError = RateLimitError;
core_APIClient.InternalServerError = InternalServerError;
core_APIClient.GatewayError = GatewayError;
core_APIClient.TimeoutError = TimeoutError;
core_APIClient.UserAbortError = APIUserAbortError;
require("crypto");
require("jsonwebtoken");
class CozeAPI extends core_APIClient {
    constructor(...args){
        super(...args), this.bots = new Bots(this), this.chat = new Chat(this), this.conversations = new Conversations(this), this.files = new Files(this), /**
   * @deprecated
   */ this.knowledge = new Knowledge(this), this.datasets = new Datasets(this), this.workflows = new Workflows(this), this.workspaces = new WorkSpaces(this), this.audio = new Audio(this), this.templates = new Templates(this), this.websockets = new Websockets(this);
    }
}
class WsSpeechClient {
    async init() {
        if (this.ws) return this.ws;
        const ws = await this.api.websockets.audio.speech.create();
        this.ws = ws;
        let isResolved = false;
        this.trackId = `my-track-id-${(0, external_uuid_namespaceObject.v4)()}`;
        this.totalDuration = 0;
        if (this.playbackTimeout) {
            clearTimeout(this.playbackTimeout);
            this.playbackTimeout = null;
        }
        this.playbackStartTime = null;
        return new Promise((resolve, reject)=>{
            ws.onopen = ()=>{
                console.debug('[speech] ws open');
            };
            ws.onmessage = (data)=>{
                // Trigger all registered event listeners
                this.emit('data', data);
                this.emit(data.event_type, data);
                if (data.event_type === types_WebsocketsEventType.ERROR) {
                    this.closeWs();
                    if (isResolved) return;
                    isResolved = true;
                    reject(new error_APIError(data.data.code, {
                        code: data.data.code,
                        msg: data.data.msg,
                        detail: data.detail
                    }, data.data.msg, void 0));
                    return;
                }
                if (data.event_type === types_WebsocketsEventType.SPEECH_CREATED) {
                    resolve(ws);
                    isResolved = true;
                } else if (data.event_type === types_WebsocketsEventType.SPEECH_AUDIO_UPDATE) this.handleAudioMessage(data.data.delta);
                else if (data.event_type === types_WebsocketsEventType.SPEECH_AUDIO_COMPLETED) {
                    console.debug('[speech] totalDuration', this.totalDuration);
                    if (this.playbackStartTime) {
                        // 剩余时间 = 总时间 - 已播放时间 - 已暂停时间
                        const now = new Date().getTime();
                        const remaining = this.totalDuration - (now - this.playbackStartTime) / 1000 - this.elapsedBeforePause;
                        this.playbackTimeout = setTimeout(()=>{
                            this.emit('completed', void 0);
                            this.playbackStartTime = null;
                            this.elapsedBeforePause = 0;
                        }, 1000 * remaining);
                    }
                    this.closeWs();
                }
            };
            ws.onerror = (error, event)=>{
                console.error('[speech] WebSocket error', error, event);
                this.emit('data', error);
                this.emit(types_WebsocketsEventType.ERROR, error);
                this.closeWs();
                if (isResolved) return;
                isResolved = true;
                reject(new error_APIError(error.data.code, error, error.data.msg, void 0));
            };
            ws.onclose = ()=>{
                console.debug('[speech] ws close');
            };
        });
    }
    async connect() {
        let { voiceId } = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : {};
        var _this_ws;
        await this.init();
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, external_uuid_namespaceObject.v4)(),
            event_type: types_WebsocketsEventType.SPEECH_UPDATE,
            data: {
                output_audio: {
                    codec: 'pcm',
                    voice_id: voiceId || void 0
                }
            }
        });
    }
    async disconnect() {
        if (this.playbackTimeout) clearTimeout(this.playbackTimeout);
        await this.wavStreamPlayer.interrupt();
        this.closeWs();
    }
    append(message) {
        var _this_ws;
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, external_uuid_namespaceObject.v4)(),
            event_type: types_WebsocketsEventType.INPUT_TEXT_BUFFER_APPEND,
            data: {
                delta: message
            }
        });
    }
    complete() {
        var _this_ws;
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, external_uuid_namespaceObject.v4)(),
            event_type: types_WebsocketsEventType.INPUT_TEXT_BUFFER_COMPLETE
        });
    }
    appendAndComplete(message) {
        this.append(message);
        this.complete();
    }
    async interrupt() {
        await this.disconnect();
        this.emit('completed', void 0);
        console.debug('[speech] playback completed', this.totalDuration);
    }
    async pause() {
        if (this.playbackTimeout) {
            clearTimeout(this.playbackTimeout);
            this.playbackTimeout = null;
        }
        if (this.playbackStartTime && !this.playbackPauseTime) {
            this.playbackPauseTime = Date.now();
            this.elapsedBeforePause += (this.playbackPauseTime - this.playbackStartTime) / 1000;
        }
        await this.wavStreamPlayer.pause();
    }
    async resume() {
        if (this.playbackPauseTime) {
            this.playbackStartTime = Date.now();
            this.playbackPauseTime = null;
            // Update the timeout with remaining duration
            if (this.playbackTimeout) clearTimeout(this.playbackTimeout);
            const remaining = this.totalDuration - this.elapsedBeforePause;
            this.playbackTimeout = setTimeout(()=>{
                this.emit('completed', void 0);
                console.debug('[speech] playback completed', this.totalDuration);
                this.playbackStartTime = null;
                this.elapsedBeforePause = 0;
            }, 1000 * remaining);
        }
        await this.wavStreamPlayer.resume();
    }
    async togglePlay() {
        if (this.isPlaying()) await this.pause();
        else await this.resume();
    }
    isPlaying() {
        return this.wavStreamPlayer.isPlaying();
    }
    on(event, callback) {
        var _this_listeners_get;
        if (!this.listeners.has(event)) this.listeners.set(event, new Set());
        null === (_this_listeners_get = this.listeners.get(event)) || void 0 === _this_listeners_get || _this_listeners_get.add(callback);
    }
    off(event, callback) {
        var _this_listeners_get;
        null === (_this_listeners_get = this.listeners.get(event)) || void 0 === _this_listeners_get || _this_listeners_get.delete(callback);
    }
    closeWs() {
        var _this_ws;
        if ((null === (_this_ws = this.ws) || void 0 === _this_ws ? void 0 : _this_ws.readyState) === 1) {
            var _this_ws1;
            null === (_this_ws1 = this.ws) || void 0 === _this_ws1 || _this_ws1.close();
        }
        this.ws = null;
    }
    emit(event, data) {
        var _this_listeners_get;
        null === (_this_listeners_get = this.listeners.get(event)) || void 0 === _this_listeners_get || _this_listeners_get.forEach((callback)=>callback(data));
    }
    constructor(config){
        this.ws = null;
        this.listeners = new Map();
        this.trackId = 'default';
        this.totalDuration = 0;
        this.playbackStartTime = null;
        this.playbackPauseTime = null;
        this.playbackTimeout = null;
        this.elapsedBeforePause = 0;
        this.handleAudioMessage = async (message)=>{
            const decodedContent = atob(message);
            const arrayBuffer = new ArrayBuffer(decodedContent.length);
            const view = new Uint8Array(arrayBuffer);
            for(let i = 0; i < decodedContent.length; i++)view[i] = decodedContent.charCodeAt(i);
            // Calculate duration in seconds
            const bytesPerSecond = 48000; // sampleRate * channels * (bitDepth/8)
            const duration = arrayBuffer.byteLength / bytesPerSecond;
            this.totalDuration += duration;
            try {
                await this.wavStreamPlayer.add16BitPCM(arrayBuffer, this.trackId);
                // Start or update the playback timer
                if (!this.playbackStartTime && !this.playbackPauseTime) {
                    this.playbackStartTime = Date.now();
                    this.elapsedBeforePause = 0;
                }
            } catch (error) {
                console.warn('[speech] wavStreamPlayer error', error);
            }
        };
        this.api = new CozeAPI({
            ...config
        });
        this.wavStreamPlayer = new WavStreamPlayer({
            sampleRate: 24000
        });
    }
}
/* ESM default export */ const speech = WsSpeechClient;
const esm_namespaceObject = require("agora-rtc-sdk-ng/esm");
const external_agora_rte_extension_namespaceObject = require("agora-rte-extension");
const WavProcessorWorklet = `
class WavProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
    this.port.onmessage = this.handleMessage.bind(this);
    this.initialize();
  }

  initialize() {
    this.chunks = [];
    this.isRecording = false;
  }

  handleMessage(event) {
    const { type } = event.data;

    switch (type) {
      case 'start':
        this.isRecording = true;
        break;
      case 'stop':
        if (this.isRecording) {
          this.isRecording = false;
          const audioData = this.processChunks();
          this.port.postMessage({
            type: 'audio',
            audioData,
            sampleRate: sampleRate,
            numChannels: this.chunks[0]?.length || 1,
          });
          this.initialize();
        }
        break;
    }
  }

  processChunks() {
    // Combine all channels
    const channels = [];
    const firstChunk = this.chunks[0] || [];
    const numChannels = firstChunk.length || 1;

    for (let channel = 0; channel < numChannels; channel++) {
      const length = this.chunks.reduce((sum, chunk) => sum + chunk[channel].length, 0);
      const channelData = new Float32Array(length);
      let offset = 0;

      for (const chunk of this.chunks) {
        channelData.set(chunk[channel], offset);
        offset += chunk[channel].length;
      }

      channels.push(channelData);
    }

    // Interleave channels
    const interleaved = new Float32Array(channels[0].length * channels.length);
    for (let i = 0; i < channels[0].length; i++) {
      for (let channel = 0; channel < channels.length; channel++) {
        interleaved[i * channels.length + channel] = channels[channel][i];
      }
    }

    return interleaved;
  }

  process(inputs) {
    const input = inputs[0];
    if (input && input[0] && this.isRecording) {
      // Clone the input data
      const chunk = input.map(channel => channel.slice());
      this.chunks.push(chunk);
    }
    return true;
  }
}

registerProcessor('wav-processor', WavProcessor);
`;
const wav_worklet_processor_script = new Blob([
    WavProcessorWorklet
], {
    type: 'application/javascript'
});
const wav_worklet_processor_src = URL.createObjectURL(wav_worklet_processor_script);
const WavProcessorSrc = wav_worklet_processor_src;
class WavAudioProcessor extends external_agora_rte_extension_namespaceObject.AudioProcessor {
    async onNode(node, context) {
        const audioContext = context.getAudioContext();
        await audioContext.audioWorklet.addModule(WavProcessorSrc);
        this.workletNode = new window.AudioWorkletNode(audioContext, 'wav-processor');
        null == node || node.connect(this.workletNode);
        this.workletNode.port.onmessage = (event)=>{
            if ('audio' === event.data.type) {
                var _this_onAudioData, _this;
                const { audioData, sampleRate, numChannels } = event.data;
                const wavBlob = this.createWavFile(audioData, sampleRate, numChannels);
                console.log('[wav-audio-processor] onAudioData', event.data);
                null === (_this_onAudioData = (_this = this).onAudioData) || void 0 === _this_onAudioData || _this_onAudioData.call(_this, {
                    wav: wavBlob
                });
            }
        };
        this.startRecording();
        this.output(node, context);
    }
    startRecording() {
        var _this_workletNode;
        null === (_this_workletNode = this.workletNode) || void 0 === _this_workletNode || _this_workletNode.port.postMessage({
            type: 'start'
        });
    }
    stopRecording() {
        var _this_workletNode;
        null === (_this_workletNode = this.workletNode) || void 0 === _this_workletNode || _this_workletNode.port.postMessage({
            type: 'stop'
        });
    }
    createWavFile(audioData, sampleRate, numChannels) {
        const buffer = floatTo16BitPCM(audioData);
        // const dataView = new DataView(buffer);
        const wavBuffer = new ArrayBuffer(44 + buffer.byteLength);
        const view = new DataView(wavBuffer);
        // Write WAV header
        const writeString = (view2, offset, string)=>{
            for(let i = 0; i < string.length; i++)view2.setUint8(offset + i, string.charCodeAt(i));
        };
        // RIFF identifier
        writeString(view, 0, 'RIFF');
        // File length
        view.setUint32(4, 36 + buffer.byteLength, true);
        // RIFF type
        writeString(view, 8, 'WAVE');
        // Format chunk identifier
        writeString(view, 12, 'fmt ');
        // Format chunk length
        view.setUint32(16, 16, true);
        // Sample format (raw)
        view.setUint16(20, 1, true);
        // Channel count
        view.setUint16(22, numChannels, true);
        // Sample rate
        view.setUint32(24, sampleRate, true);
        // Byte rate (sample rate * block align)
        view.setUint32(28, sampleRate * numChannels * 2, true);
        // Block align (channel count * bytes per sample)
        view.setUint16(32, 2 * numChannels, true);
        // Bits per sample
        view.setUint16(34, 16, true);
        // Data chunk identifier
        writeString(view, 36, 'data');
        // Data chunk length
        view.setUint32(40, buffer.byteLength, true);
        // Write audio data
        const uint8Array = new Uint8Array(buffer);
        const wavUint8Array = new Uint8Array(wavBuffer);
        wavUint8Array.set(uint8Array, 44);
        return new Blob([
            wavBuffer
        ], {
            type: 'audio/wav'
        });
    }
    /**
   * en: Destroy and cleanup resources
   * zh: 销毁并清理资源
   */ destroy() {
        var _this_workletNode;
        // 1. 停止录音
        this.stopRecording();
        // 2. 移除 workletNode 的消息监听器
        if (null === (_this_workletNode = this.workletNode) || void 0 === _this_workletNode ? void 0 : _this_workletNode.port) this.workletNode.port.onmessage = null;
        // 3. 断开 workletNode 的连接
        if (this.workletNode) {
            this.workletNode.disconnect();
            this.workletNode = void 0;
        }
        // 4. 释放 Blob URL
        if (WavProcessorSrc) URL.revokeObjectURL(WavProcessorSrc);
        // 5. 清理回调函数
        this.onAudioData = void 0;
    }
    constructor(onAudioData){
        super();
        this.name = 'WavAudioProcessor';
        this.onAudioData = onAudioData;
    }
}
/* ESM default export */ const wav_audio_processor = WavAudioProcessor;
const pcm_worklet_processor_AudioProcessorWorklet = `
class PCMProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
    this.port.onmessage = this.handleMessage.bind(this);
    this.initialize();
  }

  initialize() {
    this.buffer = [];
    this.bufferSize = 1024;
    this.isRecording = false;
  }

  handleMessage(event) {
    const { type } = event.data;

    switch (type) {
      case 'start':
        this.isRecording = true;
        break;
      case 'stop':
        this.isRecording = false;
        break;
    }
  }

  process(inputs) {
    const input = inputs[0];
    if (input.length > 0 && this.isRecording) {
      // 将当前输入添加到缓冲区
      this.buffer = this.buffer.concat(Array.from(input[0]));

      // 当缓冲区达到或超过目标大小时发送数据
      if (this.buffer.length >= this.bufferSize) {
        // 发送1024个字节的数据
        this.port.postMessage({
          audioData: new Float32Array(this.buffer),
        });

        // 清空缓冲区
        this.buffer = [];
      }
    }
    return true;
  }
}

registerProcessor('pcm-processor', PCMProcessor);
`;
const pcm_worklet_processor_script = new Blob([
    pcm_worklet_processor_AudioProcessorWorklet
], {
    type: 'application/javascript'
});
const pcm_worklet_processor_src = URL.createObjectURL(pcm_worklet_processor_script);
const pcm_worklet_processor_AudioProcessorSrc = pcm_worklet_processor_src;
class PcmAudioProcessor extends external_agora_rte_extension_namespaceObject.AudioProcessor {
    async onNode(node, context) {
        const audioContext = context.getAudioContext();
        await audioContext.audioWorklet.addModule(pcm_worklet_processor_AudioProcessorSrc);
        this.workletNode = new AudioWorkletNode(audioContext, 'pcm-processor');
        null == node || node.connect(this.workletNode);
        // workletNode.connect(node);
        this.workletNode.port.onmessage = (event)=>{
            var _this_chunkProcessor, _this;
            null === (_this_chunkProcessor = (_this = this).chunkProcessor) || void 0 === _this_chunkProcessor || _this_chunkProcessor.call(_this, floatTo16BitPCM(event.data.audioData));
        };
        this.startRecording();
        this.output(node, context);
    }
    startRecording() {
        var _this_workletNode;
        null === (_this_workletNode = this.workletNode) || void 0 === _this_workletNode || _this_workletNode.port.postMessage({
            type: 'start'
        });
    }
    stopRecording() {
        var _this_workletNode;
        null === (_this_workletNode = this.workletNode) || void 0 === _this_workletNode || _this_workletNode.port.postMessage({
            type: 'stop'
        });
    }
    /**
   * en: Destroy and cleanup resources
   * zh: 销毁并清理资源
   */ destroy() {
        var _this_workletNode;
        // 1. 停止录音
        this.stopRecording();
        // 2. 移除 workletNode 的消息监听器
        if (null === (_this_workletNode = this.workletNode) || void 0 === _this_workletNode ? void 0 : _this_workletNode.port) this.workletNode.port.onmessage = null;
        // 3. 断开 workletNode 的连接
        if (this.workletNode) {
            this.workletNode.disconnect();
            this.workletNode = void 0;
        }
        // 4. 释放 Blob URL
        if (pcm_worklet_processor_AudioProcessorSrc) URL.revokeObjectURL(pcm_worklet_processor_AudioProcessorSrc);
        // 5. 清理回调函数
        this.chunkProcessor = void 0;
    }
    constructor(chunkProcessor){
        super();
        this.name = 'PcmAudioProcessor';
        this.chunkProcessor = chunkProcessor;
    }
}
/* ESM default export */ const pcm_audio_processor = PcmAudioProcessor;
var pcm_recorder_AIDenoiserProcessorMode = /*#__PURE__*/ function(AIDenoiserProcessorMode) {
    AIDenoiserProcessorMode["NSNG"] = "NSNG";
    AIDenoiserProcessorMode["STATIONARY_NS"] = "STATIONARY_NS";
    return AIDenoiserProcessorMode;
}({});
var pcm_recorder_AIDenoiserProcessorLevel = /*#__PURE__*/ function(AIDenoiserProcessorLevel) {
    AIDenoiserProcessorLevel["SOFT"] = "SOFT";
    AIDenoiserProcessorLevel["AGGRESSIVE"] = "AGGRESSIVE";
    return AIDenoiserProcessorLevel;
}({});
class PcmRecorder {
    async start() {
        const { deviceId, mediaStreamTrack, audioCaptureConfig, wavRecordConfig, debug } = this.config;
        if (mediaStreamTrack) this.audioTrack = await (0, esm_namespaceObject.createCustomAudioTrack)({
            mediaStreamTrack
        });
        else {
            var _audioCaptureConfig_echoCancellation, _audioCaptureConfig_noiseSuppression, _audioCaptureConfig_autoGainControl;
            // Get microphone audio track
            // See:https://api-ref.agora.io/en/video-sdk/web/4.x/interfaces/microphoneaudiotrackinitconfig.html
            this.audioTrack = await (0, esm_namespaceObject.createMicrophoneAudioTrack)({
                AEC: null === (_audioCaptureConfig_echoCancellation = null == audioCaptureConfig ? void 0 : audioCaptureConfig.echoCancellation) || void 0 === _audioCaptureConfig_echoCancellation || _audioCaptureConfig_echoCancellation,
                ANS: null === (_audioCaptureConfig_noiseSuppression = null == audioCaptureConfig ? void 0 : audioCaptureConfig.noiseSuppression) || void 0 === _audioCaptureConfig_noiseSuppression || _audioCaptureConfig_noiseSuppression,
                AGC: null === (_audioCaptureConfig_autoGainControl = null == audioCaptureConfig ? void 0 : audioCaptureConfig.autoGainControl) || void 0 === _audioCaptureConfig_autoGainControl || _audioCaptureConfig_autoGainControl,
                microphoneId: deviceId,
                encoderConfig: {
                    sampleRate: 44100
                }
            });
        }
        this.stream = new window.MediaStream([
            this.audioTrack.getMediaStreamTrack()
        ]);
        // 降噪前音频
        if (debug && (null == wavRecordConfig ? void 0 : wavRecordConfig.enableSourceRecord)) this.wavAudioProcessor = new wav_audio_processor((audioData)=>{
            var _this_wavAudioCallback, _this;
            null === (_this_wavAudioCallback = (_this = this).wavAudioCallback) || void 0 === _this_wavAudioCallback || _this_wavAudioCallback.call(_this, audioData.wav, 'source');
        });
        // 降噪后音频
        if (debug && (null == wavRecordConfig ? void 0 : wavRecordConfig.enableDenoiseRecord)) this.wavAudioProcessor2 = new wav_audio_processor((audioData)=>{
            var _this_wavAudioCallback, _this;
            null === (_this_wavAudioCallback = (_this = this).wavAudioCallback) || void 0 === _this_wavAudioCallback || _this_wavAudioCallback.call(_this, audioData.wav, 'denoise');
        });
        // pcm 音频处理
        this.pcmAudioProcessor = new pcm_audio_processor((data)=>{
            var _this_pcmAudioCallback, _this;
            null === (_this_pcmAudioCallback = (_this = this).pcmAudioCallback) || void 0 === _this_pcmAudioCallback || _this_pcmAudioCallback.call(_this, {
                raw: data
            });
        });
        let audioProcessor;
        if (this.isSupportAIDenoiser()) {
            if (!PcmRecorder.denoiser) return;
            this.log('support ai denoiser');
            this.processor = PcmRecorder.denoiser.createProcessor();
            audioProcessor = this.wavAudioProcessor ? this.audioTrack.pipe(this.wavAudioProcessor).pipe(this.processor) : this.audioTrack.pipe(this.processor);
            audioProcessor = audioProcessor.pipe(this.pcmAudioProcessor);
            if (this.wavAudioProcessor2) audioProcessor = audioProcessor.pipe(this.wavAudioProcessor2);
            audioProcessor.pipe(this.audioTrack.processorDestination);
            this.handleProcessor();
        } else {
            audioProcessor = this.audioTrack.pipe(this.pcmAudioProcessor);
            if (this.wavAudioProcessor) audioProcessor = audioProcessor.pipe(this.wavAudioProcessor);
            audioProcessor.pipe(this.audioTrack.processorDestination);
        }
    }
    record() {
        let { pcmAudioCallback, wavAudioCallback, dumpAudioCallback } = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : {};
        if (!this.audioTrack || !this.stream) throw new Error('audioTrack is not initialized');
        if (this.isSupportAIDenoiser() && !this.processor) throw new Error('processor is not initialized');
        this.pcmAudioCallback = pcmAudioCallback;
        this.wavAudioCallback = wavAudioCallback;
        this.dumpAudioCallback = dumpAudioCallback;
        this.recording = true;
    }
    async handleProcessor() {
        if (!this.processor) return;
        await this.processor.enable();
        this.processor.on('overload', async ()=>{
            var _this_processor;
            console.warn('denoiser processor overload');
            // 调整为稳态降噪模式，临时关闭 AI 降噪
            await (null === (_this_processor = this.processor) || void 0 === _this_processor ? void 0 : _this_processor.setMode("STATIONARY_NS"));
        // 完全关闭 AI 降噪，使用浏览器自带的降噪
        // await processor.disable()
        });
        this.processor.on('dump', (blob, name)=>{
            var _this_dumpAudioCallback, _this;
            null === (_this_dumpAudioCallback = (_this = this).dumpAudioCallback) || void 0 === _this_dumpAudioCallback || _this_dumpAudioCallback.call(_this, blob, name);
        // const objectURL = URL.createObjectURL(blob);
        // const tag = document.createElement('a');
        // tag.download = name;
        // tag.href = objectURL;
        // tag.click();
        // setTimeout(() => {
        //   URL.revokeObjectURL(objectURL);
        // }, 0);
        });
        this.processor.on('dumpend', ()=>{
            this.log('dump ended!!');
        });
    }
    /**
   * en: Pause audio recording temporarily
   * zh: 暂时暂停音频录制
   */ pause() {
        if (this.recording) {
            var _this_wavAudioProcessor, _this_wavAudioProcessor2, _this_pcmAudioProcessor;
            // 1. 暂停音频轨道，而不是关闭它
            if (this.audioTrack) {
                const mediaStreamTrack = this.audioTrack.getMediaStreamTrack();
                mediaStreamTrack.enabled = false; // 暂停音频采集
            }
            null === (_this_wavAudioProcessor = this.wavAudioProcessor) || void 0 === _this_wavAudioProcessor || _this_wavAudioProcessor.stopRecording();
            null === (_this_wavAudioProcessor2 = this.wavAudioProcessor2) || void 0 === _this_wavAudioProcessor2 || _this_wavAudioProcessor2.stopRecording();
            null === (_this_pcmAudioProcessor = this.pcmAudioProcessor) || void 0 === _this_pcmAudioProcessor || _this_pcmAudioProcessor.stopRecording();
            // 3. 更新录制状态
            this.recording = false;
        } else this.warn('error: recorder is not recording');
    }
    /**
   * en: Resume audio recording
   * zh: 恢复音频录制
   */ resume() {
        if (!this.recording && this.audioTrack) {
            var _this_wavAudioProcessor, _this_wavAudioProcessor2, _this_pcmAudioProcessor;
            // 1. 重新启用音频轨道
            const mediaStreamTrack = this.audioTrack.getMediaStreamTrack();
            mediaStreamTrack.enabled = true; // 恢复音频采集
            null === (_this_wavAudioProcessor = this.wavAudioProcessor) || void 0 === _this_wavAudioProcessor || _this_wavAudioProcessor.startRecording();
            null === (_this_wavAudioProcessor2 = this.wavAudioProcessor2) || void 0 === _this_wavAudioProcessor2 || _this_wavAudioProcessor2.startRecording();
            null === (_this_pcmAudioProcessor = this.pcmAudioProcessor) || void 0 === _this_pcmAudioProcessor || _this_pcmAudioProcessor.startRecording();
            // 3. 更新录制状态
            this.recording = true;
        } else this.warn('recorder is recording');
    }
    /**
   * en: Destroy and cleanup all resources
   * zh: 销毁并清理所有资源
   */ destroy() {
        var _this_wavAudioProcessor, _this_wavAudioProcessor2, _this_pcmAudioProcessor;
        null === (_this_wavAudioProcessor = this.wavAudioProcessor) || void 0 === _this_wavAudioProcessor || _this_wavAudioProcessor.destroy();
        null === (_this_wavAudioProcessor2 = this.wavAudioProcessor2) || void 0 === _this_wavAudioProcessor2 || _this_wavAudioProcessor2.destroy();
        null === (_this_pcmAudioProcessor = this.pcmAudioProcessor) || void 0 === _this_pcmAudioProcessor || _this_pcmAudioProcessor.destroy();
        // 2. 关闭并清理音频轨道
        if (this.audioTrack) {
            this.audioTrack.close();
            this.audioTrack = void 0;
        }
        // 3. 停止并清理媒体流
        if (this.stream) {
            this.stream.getTracks().forEach((track)=>track.stop());
            this.stream = void 0;
        }
        // 4. 清理 AI 降噪处理器
        if (this.processor) {
            // 移除事件监听器
            this.processor.removeAllListeners();
            // 禁用处理器
            this.processor.disable();
            this.processor = void 0;
        }
        this.pcmAudioCallback = void 0;
        this.wavAudioCallback = void 0;
        this.dumpAudioCallback = void 0;
        // 5. 重置录音状态
        this.recording = false;
    }
    getStatus() {
        if (this.recording) return 'recording';
        return 'ended';
    }
    getDenoiserEnabled() {
        var _this_processor;
        return null === (_this_processor = this.processor) || void 0 === _this_processor ? void 0 : _this_processor.enabled;
    }
    async setDenoiserEnabled(enabled) {
        if (this.checkProcessor()) {
            if (enabled) {
                var _this_processor;
                await (null === (_this_processor = this.processor) || void 0 === _this_processor ? void 0 : _this_processor.enable());
            } else {
                var _this_processor1;
                await (null === (_this_processor1 = this.processor) || void 0 === _this_processor1 ? void 0 : _this_processor1.disable());
            }
        }
    }
    async setDenoiserMode(mode) {
        if (this.checkProcessor()) {
            var _this_processor;
            await (null === (_this_processor = this.processor) || void 0 === _this_processor ? void 0 : _this_processor.setMode(mode));
        }
    }
    async setDenoiserLevel(level) {
        if (this.checkProcessor()) {
            var _this_processor;
            await (null === (_this_processor = this.processor) || void 0 === _this_processor ? void 0 : _this_processor.setLevel(level));
        }
    }
    /**
   * 导出降噪处理过程中的音频数据文件
   */ dump() {
        if (this.checkProcessor()) {
            var _this_processor;
            null === (_this_processor = this.processor) || void 0 === _this_processor || _this_processor.dump();
        }
    }
    /**
   * 获取音频采样率
   */ getSampleRate() {
        return 44100;
    // return this.audioTrack?.getMediaStreamTrack().getSettings().sampleRate;
    }
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    log() {
        for(var _len = arguments.length, args = new Array(_len), _key = 0; _key < _len; _key++)args[_key] = arguments[_key];
        if (this.config.debug) console.log(...args);
        return true;
    }
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    warn() {
        for(var _len = arguments.length, args = new Array(_len), _key = 0; _key < _len; _key++)args[_key] = arguments[_key];
        if (this.config.debug) console.warn(...args);
        return true;
    }
    checkProcessor() {
        if (!this.processor) {
            // throw new Error('processor is not initialized');
            this.log('processor is not initialized');
            return false;
        }
        return true;
    }
    isSupportAIDenoiser() {
        var _this_config_aiDenoisingConfig;
        return (null === (_this_config_aiDenoisingConfig = this.config.aiDenoisingConfig) || void 0 === _this_config_aiDenoisingConfig ? void 0 : _this_config_aiDenoisingConfig.mode) && void 0 !== PcmRecorder.denoiser;
    }
    constructor(config){
        var _config_aiDenoisingConfig;
        this.recording = false;
        var _config_audioCaptureConfig;
        config.audioCaptureConfig = null !== (_config_audioCaptureConfig = config.audioCaptureConfig) && void 0 !== _config_audioCaptureConfig ? _config_audioCaptureConfig : {};
        var _config_aiDenoisingConfig1;
        config.aiDenoisingConfig = null !== (_config_aiDenoisingConfig1 = config.aiDenoisingConfig) && void 0 !== _config_aiDenoisingConfig1 ? _config_aiDenoisingConfig1 : {};
        this.config = config;
        const { audioCaptureConfig, aiDenoisingConfig } = config;
        if (checkDenoiserSupport(null === (_config_aiDenoisingConfig = config.aiDenoisingConfig) || void 0 === _config_aiDenoisingConfig ? void 0 : _config_aiDenoisingConfig.assetsPath)) {
            PcmRecorder.aiDenoiserSupport = true;
            PcmRecorder.denoiser = window.__denoiser;
        }
        if ((null == aiDenoisingConfig ? void 0 : aiDenoisingConfig.mode) && PcmRecorder.aiDenoiserSupport) {
            // 同时使用两种降噪方案时，则强制开启音频增益控制和禁用自动噪声抑制
            audioCaptureConfig.autoGainControl = true;
            audioCaptureConfig.noiseSuppression = false;
        }
    }
}
PcmRecorder.aiDenoiserSupport = false;
/* ESM default export */ const pcm_recorder = PcmRecorder;
class BaseWsTranscriptionClient {
    async init() {
        if (this.ws) return this.ws;
        const ws = await this.api.websockets.audio.transcriptions.create(this.config.websocketOptions);
        let isResolved = false;
        return new Promise((resolve, reject)=>{
            ws.onopen = ()=>{
                console.debug('[transcription] ws open');
            };
            ws.onmessage = (data)=>{
                // Trigger all registered event listeners
                this.emit(types_WebsocketsEventType.ALL, data);
                this.emit(data.event_type, data);
                if (data.event_type === types_WebsocketsEventType.ERROR) {
                    this.closeWs();
                    if (isResolved) return;
                    isResolved = true;
                    reject(new error_APIError(data.data.code, {
                        code: data.data.code,
                        msg: data.data.msg,
                        detail: data.detail
                    }, data.data.msg, void 0));
                    return;
                }
                if (data.event_type === types_WebsocketsEventType.TRANSCRIPTIONS_CREATED) {
                    resolve(ws);
                    isResolved = true;
                } else if (data.event_type === types_WebsocketsEventType.TRANSCRIPTIONS_MESSAGE_COMPLETED) this.closeWs();
            };
            ws.onerror = (error, event)=>{
                console.error('[transcription] WebSocket error', error, event);
                this.emit('data', error);
                this.emit(types_WebsocketsEventType.ERROR, error);
                this.closeWs();
                if (isResolved) return;
                isResolved = true;
                reject(new error_APIError(error.data.code, error, error.data.msg, void 0));
            };
            ws.onclose = ()=>{
                console.debug('[transcription] ws close');
            };
            this.ws = ws;
        });
    }
    /**
   * 监听一个或多个事件
   * @param event 事件名称或事件名称数组
   * @param callback 回调函数
   */ on(event, callback) {
        const events = Array.isArray(event) ? event : [
            event
        ];
        events.forEach((eventName)=>{
            var _this_listeners_get;
            if (!this.listeners.has(eventName)) this.listeners.set(eventName, new Set());
            null === (_this_listeners_get = this.listeners.get(eventName)) || void 0 === _this_listeners_get || _this_listeners_get.add(callback);
        });
    }
    /**
   * 移除一个或多个事件的监听
   * @param event 事件名称或事件名称数组
   * @param callback 回调函数
   */ off(event, callback) {
        const events = Array.isArray(event) ? event : [
            event
        ];
        events.forEach((eventName)=>{
            var _this_listeners_get;
            null === (_this_listeners_get = this.listeners.get(eventName)) || void 0 === _this_listeners_get || _this_listeners_get.delete(callback);
        });
    }
    closeWs() {
        var _this_ws;
        if ((null === (_this_ws = this.ws) || void 0 === _this_ws ? void 0 : _this_ws.readyState) === 1) {
            var _this_ws1;
            null === (_this_ws1 = this.ws) || void 0 === _this_ws1 || _this_ws1.close();
        }
        this.ws = null;
    }
    emit(event, data) {
        var _this_listeners_get;
        null === (_this_listeners_get = this.listeners.get(event)) || void 0 === _this_listeners_get || _this_listeners_get.forEach((callback)=>callback(data));
    }
    constructor(config){
        this.ws = null;
        this.listeners = new Map();
        this.api = new CozeAPI({
            ...config,
            debug: false
        });
        this.recorder = new pcm_recorder({
            audioCaptureConfig: config.audioCaptureConfig,
            aiDenoisingConfig: config.aiDenoisingConfig,
            mediaStreamTrack: config.mediaStreamTrack,
            wavRecordConfig: config.wavRecordConfig,
            deviceId: config.deviceId || 'default',
            debug: config.debug
        });
        this.config = config;
    }
}
/* ESM default export */ const base = BaseWsTranscriptionClient;
class WsTranscriptionClient extends base {
    async connect() {
        var _this_ws;
        await this.init();
        await this.recorder.start();
        const sampleRate = this.recorder.getSampleRate();
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, external_uuid_namespaceObject.v4)(),
            event_type: types_WebsocketsEventType.TRANSCRIPTIONS_UPDATE,
            data: {
                input_audio: {
                    format: 'pcm',
                    codec: 'pcm',
                    sample_rate: sampleRate,
                    channel: 1,
                    bit_depth: 16
                }
            }
        });
    }
    destroy() {
        this.recorder.destroy();
        this.listeners.clear();
        this.closeWs();
    }
    getStatus() {
        if (this.isRecording) {
            if ('ended' === this.recorder.getStatus()) return 'paused';
            return 'recording';
        }
        return 'ended';
    }
    async start() {
        if ('recording' === this.getStatus()) {
            console.warn('Recording is already started');
            return;
        }
        await this.connect();
        await this.recorder.record({
            pcmAudioCallback: (data)=>{
                var _this_ws;
                const { raw } = data;
                // Convert ArrayBuffer to base64 string
                const base64String = btoa(Array.from(new Uint8Array(raw)).map((byte)=>String.fromCharCode(byte)).join(''));
                null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
                    id: (0, external_uuid_namespaceObject.v4)(),
                    event_type: types_WebsocketsEventType.INPUT_AUDIO_BUFFER_APPEND,
                    data: {
                        delta: base64String
                    }
                });
            }
        });
        this.isRecording = true;
    }
    /**
   * 停止录音，提交结果
   */ stop() {
        var _this_ws;
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, external_uuid_namespaceObject.v4)(),
            event_type: types_WebsocketsEventType.INPUT_AUDIO_BUFFER_COMPLETE
        });
        this.recorder.destroy();
        this.closeWs();
        this.isRecording = false;
    }
    /**
   * 暂停录音（保留上下文）
   */ pause() {
        if ('recording' !== this.getStatus()) throw new Error('Recording is not started');
        return this.recorder.pause();
    }
    /**
   * 恢复录音
   */ resume() {
        if ('paused' !== this.getStatus()) throw new Error('Recording is not paused');
        return this.recorder.resume();
    }
    getDenoiserEnabled() {
        return this.recorder.getDenoiserEnabled();
    }
    setDenoiserEnabled(enabled) {
        return this.recorder.setDenoiserEnabled(enabled);
    }
    setDenoiserMode(mode) {
        return this.recorder.setDenoiserMode(mode);
    }
    setDenoiserLevel(level) {
        return this.recorder.setDenoiserLevel(level);
    }
    constructor(...args){
        super(...args), this.isRecording = false;
    }
}
/* ESM default export */ const transcription = WsTranscriptionClient;
var types_WsChatEventNames = /*#__PURE__*/ function(WsChatEventNames) {
    /**
   * en: All events
   * zh: 所有事件
   */ WsChatEventNames["ALL"] = "realtime.event";
    /**
   * en: Client connected
   * zh: 客户端连接
   */ WsChatEventNames["CONNECTED"] = "client.connected";
    /**
   * en: Client connecting
   * zh: 客户端连接中
   */ WsChatEventNames["CONNECTING"] = "client.connecting";
    /**
   * en: Client interrupted
   * zh: 客户端中断
   */ WsChatEventNames["INTERRUPTED"] = "client.interrupted";
    /**
   * en: Client disconnected
   * zh: 客户端断开
   */ WsChatEventNames["DISCONNECTED"] = "client.disconnected";
    /**
   * en: Client audio unmuted
   * zh: 客户端音频未静音
   */ WsChatEventNames["AUDIO_UNMUTED"] = "client.audio.unmuted";
    /**
   * en: Client audio muted
   * zh: 客户端音频静音
   */ WsChatEventNames["AUDIO_MUTED"] = "client.audio.muted";
    /**
   * en: Client error
   * zh: 客户端错误
   */ WsChatEventNames["ERROR"] = "client.error";
    /**
   * en: Denoiser enabled
   * zh: 降噪开启
   */ WsChatEventNames["DENOISER_ENABLED"] = "client.denoiser.enabled";
    /**
   * en: Denoiser disabled
   * zh: 降噪关闭
   */ WsChatEventNames["DENOISER_DISABLED"] = "client.denoiser.disabled";
    /**
   * en: Audio input device changed
   * zh: 音频输入设备改变
   */ WsChatEventNames["AUDIO_INPUT_DEVICE_CHANGED"] = "client.input.device.changed";
    /**
   * en: Audio output device changed
   * zh: 音频输出设备改变
   */ WsChatEventNames["AUDIO_OUTPUT_DEVICE_CHANGED"] = "client.output.device.changed";
    /**
   * en: Audio record dump
   * zh: 音频 dump
   */ WsChatEventNames["AUDIO_INPUT_DUMP"] = "client.audio.input.dump";
    /**
   * en: Chat created
   * zh: 对话创建成功
   */ WsChatEventNames["CHAT_CREATED"] = "server.chat.created";
    /**
   * en: Chat updated
   * zh: 对话更新
   */ WsChatEventNames["CHAT_UPDATED"] = "server.chat.updated";
    /**
   * en: Conversation chat created
   * zh: 会话对话创建
   */ WsChatEventNames["CONVERSATION_CHAT_CREATED"] = "server.conversation.chat.created";
    /**
   * en: Conversation chat in progress
   * zh: 对话正在处理中
   */ WsChatEventNames["CONVERSATION_CHAT_IN_PROGRESS"] = "server.conversation.chat.in.progress";
    /**
   * en: Conversation message delta received
   * zh: 文本消息增量返回
   */ WsChatEventNames["CONVERSATION_MESSAGE_DELTA"] = "server.conversation.message.delta";
    /**
   * en: Conversation audio delta received
   * zh: 语音消息增量返回
   */ WsChatEventNames["CONVERSATION_AUDIO_DELTA"] = "server.conversation.audio.delta";
    /**
   * en: Conversation message completed
   * zh: 文本消息完成
   */ WsChatEventNames["CONVERSATION_MESSAGE_COMPLETED"] = "server.conversation.message.completed";
    /**
   * en: Conversation audio completed
   * zh: 语音回复完成
   */ WsChatEventNames["CONVERSATION_AUDIO_COMPLETED"] = "server.conversation.audio.completed";
    /**
   * en: Conversation chat completed
   * zh: 对话完成
   */ WsChatEventNames["CONVERSATION_CHAT_COMPLETED"] = "server.conversation.chat.completed";
    /**
   * en: Conversation chat failed
   * zh: 对话失败
   */ WsChatEventNames["CONVERSATION_CHAT_FAILED"] = "server.conversation.chat.failed";
    /**
   * en: Server error occurred
   * zh: 服务端错误
   */ WsChatEventNames["SERVER_ERROR"] = "server.error";
    /**
   * en: Input audio buffer completed
   * zh: 语音输入缓冲区提交完成
   */ WsChatEventNames["INPUT_AUDIO_BUFFER_COMPLETED"] = "server.input_audio_buffer.completed";
    /**
   * en: Input audio buffer cleared
   * zh: 语音输入缓冲区已清除
   */ WsChatEventNames["INPUT_AUDIO_BUFFER_CLEARED"] = "server.input_audio_buffer.cleared";
    /**
   * en: Conversation chat cancelled
   * zh: 对话被取消
   */ WsChatEventNames["CONVERSATION_CHAT_CANCELLED"] = "server.conversation.chat.cancelled";
    /**
   * en: Conversation context cleared
   * zh: 对话上下文已清除
   */ WsChatEventNames["CONVERSATION_CLEARED"] = "server.conversation.cleared";
    /**
   * en: Conversation audio transcript updated
   * zh: 用户语音识别实时字幕更新
   */ WsChatEventNames["CONVERSATION_AUDIO_TRANSCRIPT_UPDATE"] = "server.conversation.audio_transcript.update";
    /**
   * en: Conversation audio transcript completed
   * zh: 用户语音识别完成
   */ WsChatEventNames["CONVERSATION_AUDIO_TRANSCRIPT_COMPLETED"] = "server.conversation.audio_transcript.completed";
    /**
   * en: Conversation chat requires action
   * zh: 对话需要端插件响应
   */ WsChatEventNames["CONVERSATION_CHAT_REQUIRES_ACTION"] = "server.conversation.chat.requires_action";
    /**
   * en: User speech detected - started
   * zh: 检测到用户开始说话
   */ WsChatEventNames["INPUT_AUDIO_BUFFER_SPEECH_STARTED"] = "server.input_audio_buffer.speech_started";
    /**
   * en: User speech detected - stopped
   * zh: 检测到用户停止说话
   */ WsChatEventNames["INPUT_AUDIO_BUFFER_SPEECH_STOPPED"] = "server.input_audio_buffer.speech_stopped";
    /**
   * en: Audio dump
   * zh: 音频 dump
   */ WsChatEventNames["DUMP_AUDIO"] = "server.dump.audio";
    return WsChatEventNames;
}({});
class BaseWsChatClient {
    async init() {
        if (this.ws) return this.ws;
        const ws = await this.api.websockets.chat.create({
            bot_id: this.config.botId,
            workflow_id: this.config.workflowId
        }, this.config.websocketOptions);
        this.ws = ws;
        // 标记 websocket 是否已 resolve or reject
        let isResolved = false;
        this.trackId = `my-track-id-${(0, external_uuid_namespaceObject.v4)()}`;
        return new Promise((resolve, reject)=>{
            ws.onopen = ()=>{
                this.log('ws open');
            };
            ws.onmessage = (data)=>{
                // Trigger all registered event listeners
                this.emit(`server.${data.event_type}`, data);
                switch(data.event_type){
                    case types_WebsocketsEventType.ERROR:
                        this.closeWs();
                        if (isResolved) return;
                        isResolved = true;
                        reject(new error_APIError(data.data.code, {
                            code: data.data.code,
                            msg: data.data.msg,
                            detail: data.detail
                        }, void 0, void 0));
                        return;
                    case types_WebsocketsEventType.CHAT_CREATED:
                        resolve(ws);
                        isResolved = true;
                        break;
                    case types_WebsocketsEventType.INPUT_AUDIO_BUFFER_SPEECH_STOPPED:
                        this.complete();
                        break;
                    case types_WebsocketsEventType.CONVERSATION_AUDIO_DELTA:
                        this.audioDeltaList.push(data.data.content);
                        if (1 === this.audioDeltaList.length) this.handleAudioMessage();
                        break;
                    case types_WebsocketsEventType.INPUT_AUDIO_BUFFER_SPEECH_STARTED:
                        this.clear();
                        break;
                    case types_WebsocketsEventType.CONVERSATION_AUDIO_COMPLETED:
                        break;
                    case types_WebsocketsEventType.CONVERSATION_CHAT_CANCELED:
                        // this.isInterrupted = false;
                        this.clear();
                        break;
                    default:
                        break;
                }
            };
            ws.onerror = (error, event)=>{
                this.warn('ws error', error, event);
                this.emit(`server.${types_WebsocketsEventType.ERROR}`, error);
                this.closeWs();
                if (isResolved) return;
                isResolved = true;
                reject(new error_APIError(error.data.code, error, error.data.msg, void 0));
            };
            ws.onclose = ()=>{
                this.log('ws close');
            };
        });
    }
    sendMessage(data) {
        var _this_ws;
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send(data);
        this.log('sendMessage', data);
    }
    sendTextMessage(text) {
        this.sendMessage({
            id: (0, external_uuid_namespaceObject.v4)(),
            event_type: types_WebsocketsEventType.CONVERSATION_MESSAGE_CREATE,
            data: {
                role: chat_RoleType.User,
                content_type: 'text',
                content: text
            }
        });
    }
    /**
   * en: Add event listener(s)
   * zh: 添加事件监听器
   * @param event - string | string[] Event name or array of event names
   * @param callback - Event callback function
   */ on(event, callback) {
        const events = Array.isArray(event) ? event : [
            event
        ];
        events.forEach((eventName)=>{
            var _this_listeners_get;
            if (!this.listeners.has(eventName)) this.listeners.set(eventName, new Set());
            null === (_this_listeners_get = this.listeners.get(eventName)) || void 0 === _this_listeners_get || _this_listeners_get.add(callback);
            this.log('on', eventName);
        });
    }
    /**
   * en: Remove event listener(s)
   * zh: 移除事件监听器
   * @param event - string | string[] Event name or array of event names
   * @param callback - Event callback function to remove
   */ off(event, callback) {
        const events = Array.isArray(event) ? event : [
            event
        ];
        events.forEach((eventName)=>{
            var _this_listeners_get;
            null === (_this_listeners_get = this.listeners.get(eventName)) || void 0 === _this_listeners_get || _this_listeners_get.delete(callback);
        });
    }
    // isPlaying() {
    //   return this.wavStreamPlayer.isPlaying();
    // }
    complete() {
        var _this_ws;
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, external_uuid_namespaceObject.v4)(),
            event_type: types_WebsocketsEventType.INPUT_AUDIO_BUFFER_COMPLETE
        });
    }
    closeWs() {
        var _this_ws;
        if ((null === (_this_ws = this.ws) || void 0 === _this_ws ? void 0 : _this_ws.readyState) === 1) {
            var _this_ws1;
            null === (_this_ws1 = this.ws) || void 0 === _this_ws1 || _this_ws1.close();
        }
        this.ws = null;
    }
    async clear() {
        this.log('clear');
        this.audioDeltaList.length = 0;
        await this.wavStreamPlayer.interrupt();
        this.trackId = `my-track-id-${(0, external_uuid_namespaceObject.v4)()}`;
    }
    emit(eventName, event) {
        var _this_listeners_get, _this_listeners_get1;
        null === (_this_listeners_get = this.listeners.get(eventName)) || void 0 === _this_listeners_get || _this_listeners_get.forEach((callback)=>callback(eventName, event));
        null === (_this_listeners_get1 = this.listeners.get(types_WsChatEventNames.ALL)) || void 0 === _this_listeners_get1 || _this_listeners_get1.forEach((callback)=>callback(eventName, event));
        this.log('dispatch', eventName, event);
    }
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    log() {
        for(var _len = arguments.length, args = new Array(_len), _key = 0; _key < _len; _key++)args[_key] = arguments[_key];
        if (this.config.debug) console.log('[WsChatClient]', ...args);
        return true;
    }
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    warn() {
        for(var _len = arguments.length, args = new Array(_len), _key = 0; _key < _len; _key++)args[_key] = arguments[_key];
        if (this.config.debug) console.warn('[WsChatClient]', ...args);
        return true;
    }
    constructor(config){
        this.ws = null;
        this.listeners = new Map();
        this.trackId = 'default';
        this.audioDeltaList = [];
        this.handleAudioMessage = async ()=>{
            const message = this.audioDeltaList[0];
            const decodedContent = atob(message);
            const arrayBuffer = new ArrayBuffer(decodedContent.length);
            const view = new Uint8Array(arrayBuffer);
            for(let i = 0; i < decodedContent.length; i++)view[i] = decodedContent.charCodeAt(i);
            try {
                await this.wavStreamPlayer.add16BitPCM(arrayBuffer, this.trackId);
                this.audioDeltaList.shift();
                if (this.audioDeltaList.length > 0) this.handleAudioMessage();
            } catch (error) {
                this.warn('wavStreamPlayer error', error);
            }
        };
        this.api = new CozeAPI({
            ...config,
            debug: false
        });
        this.config = config;
        this.wavStreamPlayer = new WavStreamPlayer({
            sampleRate: 24000
        });
    }
}
/* ESM default export */ const chat_base = BaseWsChatClient;
class WsChatClient extends chat_base {
    async startRecord() {
        // 1. start recorder
        await this.recorder.start();
        // init stream player
        await this.wavStreamPlayer.add16BitPCM(new ArrayBuffer(0), this.trackId);
        // let startTime = performance.now();
        // 2. recording
        await this.recorder.record({
            pcmAudioCallback: (data)=>{
                var _this_ws;
                const { raw } = data;
                // Convert ArrayBuffer to base64 string
                const base64String = btoa(Array.from(new Uint8Array(raw)).map((byte)=>String.fromCharCode(byte)).join(''));
                null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
                    id: (0, external_uuid_namespaceObject.v4)(),
                    event_type: types_WebsocketsEventType.INPUT_AUDIO_BUFFER_APPEND,
                    data: {
                        delta: base64String
                    }
                });
            // this.log('input_audio_buffer_append', performance.now() - startTime);
            // startTime = performance.now();
            },
            wavAudioCallback: (blob, name)=>{
                const event = {
                    event_type: 'audio.input.dump',
                    data: {
                        name,
                        wav: blob
                    }
                };
                this.emit(types_WsChatEventNames.AUDIO_INPUT_DUMP, event);
            },
            dumpAudioCallback: (blob, name)=>{
                const event = {
                    event_type: 'audio.input.dump',
                    data: {
                        name,
                        wav: blob
                    }
                };
                this.emit(types_WsChatEventNames.AUDIO_INPUT_DUMP, event);
            }
        });
    }
    async connect() {
        let { chatUpdate } = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : {};
        var _this_recorder;
        const ws = await this.init();
        this.ws = ws;
        if (!this.isMuted) await this.startRecord();
        const sampleRate = await (null === (_this_recorder = this.recorder) || void 0 === _this_recorder ? void 0 : _this_recorder.getSampleRate());
        const event = {
            id: (null == chatUpdate ? void 0 : chatUpdate.id) || (0, external_uuid_namespaceObject.v4)(),
            event_type: types_WebsocketsEventType.CHAT_UPDATE,
            data: {
                input_audio: {
                    format: 'pcm',
                    codec: 'pcm',
                    sample_rate: sampleRate
                },
                output_audio: {
                    codec: 'pcm',
                    pcm_config: {
                        sample_rate: 24000
                    },
                    voice_id: this.config.voiceId || void 0
                },
                turn_detection: {
                    type: 'server_vad'
                },
                need_play_prologue: true,
                ...null == chatUpdate ? void 0 : chatUpdate.data
            }
        };
        this.ws.send(event);
        this.emit(types_WsChatEventNames.CONNECTED, event);
    }
    async disconnect() {
        var _this_recorder;
        await this.wavStreamPlayer.interrupt();
        await (null === (_this_recorder = this.recorder) || void 0 === _this_recorder ? void 0 : _this_recorder.destroy());
        this.emit(types_WsChatEventNames.DISCONNECTED, void 0);
        await new Promise((resolve)=>setTimeout(resolve, 500));
        this.listeners.clear();
        this.closeWs();
    }
    /**
   * en: Set the audio enable
   * zh: 设置是否静音
   * @param enable - The enable to set
   */ async setAudioEnable(enable) {
        var _this_recorder;
        const status = await (null === (_this_recorder = this.recorder) || void 0 === _this_recorder ? void 0 : _this_recorder.getStatus());
        if (enable) {
            if ('ended' === status) {
                if (this.recorder.audioTrack) {
                    var _this_recorder1;
                    await (null === (_this_recorder1 = this.recorder) || void 0 === _this_recorder1 ? void 0 : _this_recorder1.resume());
                } else this.startRecord();
                this.isMuted = false;
                this.emit(types_WsChatEventNames.AUDIO_UNMUTED, void 0);
            } else this.warn('recorder is not ended with status', status);
        } else if ('recording' === status) {
            await this.recorder.pause();
            this.isMuted = true;
            this.emit(types_WsChatEventNames.AUDIO_MUTED, void 0);
        } else this.warn('recorder is not recording with status', status);
    }
    /**
   * en: Set the audio input device
   * zh: 设置音频输入设备
   * @param deviceId - The device ID to set
   */ async setAudioInputDevice(deviceId) {
        if ('ended' !== this.recorder.getStatus()) await this.recorder.destroy();
        const devices = await getAudioDevices();
        if ('default' === deviceId) {
            this.recorder.config.deviceId = void 0;
            if (!this.isMuted) await this.startRecord();
            this.emit(types_WsChatEventNames.AUDIO_INPUT_DEVICE_CHANGED, void 0);
        } else {
            const device = devices.audioInputs.find((d)=>d.deviceId === deviceId);
            if (!device) throw new Error(`Device with id ${deviceId} not found`);
            this.recorder.config.deviceId = device.deviceId;
            if (!this.isMuted) await this.startRecord();
            this.emit(types_WsChatEventNames.AUDIO_INPUT_DEVICE_CHANGED, void 0);
        }
    }
    /**
   * en: Interrupt the conversation
   * zh: 打断对话
   */ interrupt() {
        var _this_ws;
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, external_uuid_namespaceObject.v4)(),
            event_type: types_WebsocketsEventType.CONVERSATION_CHAT_CANCEL
        });
        this.emit(types_WsChatEventNames.INTERRUPTED, void 0);
    }
    /**
   * en: Set the denoiser enabled
   * zh: 设置是否启用降噪
   * @param enabled - The enabled to set
   */ setDenoiserEnabled(enabled) {
        this.recorder.setDenoiserEnabled(enabled);
        if (enabled) this.emit(types_WsChatEventNames.DENOISER_ENABLED, void 0);
        else this.emit(types_WsChatEventNames.DENOISER_DISABLED, void 0);
    }
    /**
   * en: Set the denoiser level
   * zh: 设置降噪等级
   * @param level - The level to set
   */ setDenoiserLevel(level) {
        this.log('setDenoiserLevel', level);
        this.recorder.setDenoiserLevel(level);
    }
    /**
   * en: Set the denoiser mode
   * zh: 设置降噪模式
   * @param mode - The mode to set
   */ setDenoiserMode(mode) {
        this.log('setDenoiserMode', mode);
        this.recorder.setDenoiserMode(mode);
    }
    constructor(config){
        super(config), this.isMuted = false;
        this.recorder = new pcm_recorder({
            audioCaptureConfig: config.audioCaptureConfig,
            aiDenoisingConfig: config.aiDenoisingConfig,
            mediaStreamTrack: config.mediaStreamTrack,
            wavRecordConfig: config.wavRecordConfig,
            debug: config.debug,
            deviceId: config.deviceId
        });
        var _config_audioMutedDefault;
        this.isMuted = null !== (_config_audioMutedDefault = config.audioMutedDefault) && void 0 !== _config_audioMutedDefault && _config_audioMutedDefault;
    }
}
/* ESM default export */ const ws_tools_chat = WsChatClient;
var __webpack_export_target__ = exports;
for(var i in __webpack_exports__)__webpack_export_target__[i] = __webpack_exports__[i];
if (__webpack_exports__.__esModule) Object.defineProperty(__webpack_export_target__, '__esModule', {
    value: true
});
